{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90cf9dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración básica completada\n",
      "Python version: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:32:59) [MSC v.1943 32 bit (Intel)]\n",
      "Working directory: h:\\git\\SAR360-AnaliticaCrediticia\\SAR360-AnaliticaCrediticia\\notebooks\n",
      "Creando estructura de directorios...\n",
      "✓ Estructura de directorios creada\n"
     ]
    }
   ],
   "source": [
    "# Importaciones básicas con manejo de errores\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"Configuración básica completada\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Crear datos sintéticos para demostración\n",
    "def create_sample_credit_data():\n",
    "    \"\"\"Crear dataset sintético de crédito para demostración\"\"\"\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \n",
    "    sample_data = []\n",
    "    for i in range(1000):\n",
    "        record = {\n",
    "            'person_age': random.randint(18, 75),\n",
    "            'person_income': random.randint(20000, 150000),\n",
    "            'person_home_ownership': random.choice(['RENT', 'OWN', 'MORTGAGE']),\n",
    "            'person_emp_length': random.randint(0, 40),\n",
    "            'loan_intent': random.choice(['PERSONAL', 'EDUCATION', 'MEDICAL', 'VENTURE', 'HOME', 'AUTO']),\n",
    "            'loan_grade': random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G']),\n",
    "            'loan_amnt': random.randint(1000, 40000),\n",
    "            'loan_int_rate': round(random.uniform(5.42, 23.22), 2),\n",
    "            'loan_status': random.choice(['Default', 'Non Default', 'Non Default', 'Non Default']),  # 25% default\n",
    "            'loan_percent_income': round(random.uniform(0.02, 0.83), 3),\n",
    "            'cb_person_default_on_file': random.choice(['Y', 'N', 'N', 'N']),  # 25% with previous default\n",
    "            'cb_person_cred_hist_length': random.randint(1, 30)\n",
    "        }\n",
    "        sample_data.append(record)\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# Crear directorios necesarios\n",
    "print(\"Creando estructura de directorios...\")\n",
    "Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/errors\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Estructura de directorios creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51fca671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros ETL configurados:\n",
      "  - missing_values: {'drop_threshold': 0.7, 'numerical': 'median', 'categorical': 'mode'}\n",
      "  - outliers: {'method': 'iqr', 'threshold': 2.0, 'exclude_columns': ['default_flag']}\n"
     ]
    }
   ],
   "source": [
    "# Configuración de parámetros ETL (limpieza y outliers)\n",
    "ETL_CONFIG = {\n",
    "    'missing_values': {\n",
    "        'drop_threshold': 0.7,      # Eliminar columnas con >70% nulos\n",
    "        'numerical': 'median',       # 'median' o 'mean'\n",
    "        'categorical': 'mode'        # 'mode'\n",
    "    },\n",
    "    'outliers': {\n",
    "        'method': 'iqr',            # Método de detección\n",
    "        'threshold': 2.0,           # 1.5-3.0 recomendado; 2.0 conservador para finanzas\n",
    "        'exclude_columns': ['default_flag']  # Columnas a excluir del análisis de outliers\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Parámetros ETL configurados:\")\n",
    "print(\"  - missing_values:\", ETL_CONFIG['missing_values'])\n",
    "print(\"  - outliers:\", ETL_CONFIG['outliers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc849065",
   "metadata": {},
   "source": [
    "# ETL - Analítica de Riesgo Crediticio\n",
    "\n",
    "## Objetivo\n",
    "Se realizó la extracción, transformación y carga del dataset credit_risk_dataset.csv para preparar los datos para el análisis de riesgo crediticio y modelado PD/LGD/EAD.\n",
    "\n",
    "## Entregables\n",
    "- data/processed/clean_data.csv: Dataset limpio y procesado\n",
    "- reports/etl_report.md: Reporte detallado del proceso ETL\n",
    "- data/errors/etl_errors_*.csv: Registros con errores identificados\n",
    "- logs/etl_*.log: Logs de ejecución\n",
    "\n",
    "## Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2233d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando dataset sintético de riesgo crediticio...\n",
      "✓ Dataset creado: ../data/raw/credit_risk_dataset.csv\n",
      "✓ Registros generados: 5,000\n",
      "✓ Columnas: 12\n",
      "\n",
      "Estructura del dataset:\n",
      " 1. person_age\n",
      " 2. person_income\n",
      " 3. person_home_ownership\n",
      " 4. person_emp_length\n",
      " 5. loan_intent\n",
      " 6. loan_grade\n",
      " 7. loan_amnt\n",
      " 8. loan_int_rate\n",
      " 9. loan_status\n",
      "10. loan_percent_income\n",
      "11. cb_person_default_on_file\n",
      "12. cb_person_cred_hist_length\n",
      "\n",
      "Muestra de datos (primeros 3 registros):\n",
      "  Registro 1: {'person_age': 58, 'person_income': 49184, 'person_home_ownership': 'RENT', 'person_emp_length': 17, 'loan_intent': 'EDUCATION', 'loan_grade': 'B', 'loan_amnt': 10144, 'loan_int_rate': 18.53, 'loan_status': 'Default', 'loan_percent_income': 0.498, 'cb_person_default_on_file': 'Y', 'cb_person_cred_hist_length': 1}\n",
      "  Registro 2: {'person_age': 23, 'person_income': 77314, 'person_home_ownership': 'OWN', 'person_emp_length': 32, 'loan_intent': 'HOMEIMPROVEMENT', 'loan_grade': 'A', 'loan_amnt': 37781, 'loan_int_rate': 8.96, 'loan_status': 'Non Default', 'loan_percent_income': 0.199, 'cb_person_default_on_file': 'N', 'cb_person_cred_hist_length': 26}\n",
      "  Registro 3: {'person_age': 73, 'person_income': 21703, 'person_home_ownership': 'OWN', 'person_emp_length': 27, 'loan_intent': 'MEDICAL', 'loan_grade': 'C', 'loan_amnt': 11189, 'loan_int_rate': 9.25, 'loan_status': 'Non Default', 'loan_percent_income': 0.103, 'cb_person_default_on_file': 'N', 'cb_person_cred_hist_length': 4}\n",
      "[2025-10-15 15:30:17] INFO: Se generó dataset sintético con 5000 registros\n",
      "\n",
      "✓ Configuración ETL completada\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset sintético de riesgo crediticio\n",
    "sample_data = []\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Generando dataset sintético de riesgo crediticio...\")\n",
    "\n",
    "for i in range(5000):  # 5000 registros sintéticos\n",
    "    record = {\n",
    "        'person_age': random.randint(18, 75),\n",
    "        'person_income': random.randint(20000, 200000),\n",
    "        'person_home_ownership': random.choice(['RENT', 'OWN', 'MORTGAGE', 'OTHER']),\n",
    "        'person_emp_length': random.randint(0, 40),\n",
    "        'loan_intent': random.choice(['PERSONAL', 'EDUCATION', 'MEDICAL', 'VENTURE', 'HOMEIMPROVEMENT', 'DEBTCONSOLIDATION']),\n",
    "        'loan_grade': random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G']),\n",
    "        'loan_amnt': random.randint(1000, 40000),\n",
    "        'loan_int_rate': round(random.uniform(5.42, 23.22), 2),\n",
    "        'loan_status': random.choice(['Default', 'Non Default', 'Non Default', 'Non Default']),  # ~25% default\n",
    "        'loan_percent_income': round(random.uniform(0.02, 0.83), 3),\n",
    "        'cb_person_default_on_file': random.choice(['Y', 'N', 'N', 'N']),  # ~25% previous default\n",
    "        'cb_person_cred_hist_length': random.randint(1, 30)\n",
    "    }\n",
    "    sample_data.append(record)\n",
    "\n",
    "# Guardar dataset sintético\n",
    "DATA_RAW_PATH = \"../data/raw/credit_risk_dataset.csv\"\n",
    "\n",
    "with open(DATA_RAW_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = sample_data[0].keys()\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "print(f\"✓ Dataset creado: {DATA_RAW_PATH}\")\n",
    "print(f\"✓ Registros generados: {len(sample_data):,}\")\n",
    "print(f\"✓ Columnas: {len(fieldnames):,}\")\n",
    "\n",
    "# Verificar estructura\n",
    "print(f\"\\nEstructura del dataset:\")\n",
    "for i, field in enumerate(fieldnames, 1):\n",
    "    print(f\"{i:2d}. {field}\")\n",
    "\n",
    "print(f\"\\nMuestra de datos (primeros 3 registros):\")\n",
    "for i in range(3):\n",
    "    print(f\"  Registro {i+1}: {sample_data[i]}\")\n",
    "\n",
    "# Configurar logging básico\n",
    "def log_operation(message, level=\"INFO\", data=None):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {level}: {message}\"\n",
    "    if data:\n",
    "        log_entry += f\" | Data: {data}\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # Guardar en archivo log\n",
    "    log_file = f\"../logs/etl_{datetime.now().strftime('%Y%m%d')}.log\"\n",
    "    Path(\"../logs\").mkdir(exist_ok=True)\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry + \"\\n\")\n",
    "\n",
    "log_operation(f\"Se generó dataset sintético con {len(sample_data)} registros\")\n",
    "print(\"\\n✓ Configuración ETL completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1e3b9",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90c4eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo encontrado: ../data/raw/credit_risk_dataset.csv\n",
      "Se verificó la presencia del dataset de riesgo crediticio\n",
      "Tamaño del archivo: 321.6 KB\n",
      "[2025-10-15 15:30:22] INFO: Archivo verificado: ../data/raw/credit_risk_dataset.csv (329287 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Se configuraron rutas\n",
    "DATA_RAW_PATH = \"../data/raw/credit_risk_dataset.csv\"\n",
    "DATA_PROCESSED_PATH = \"../data/processed\"\n",
    "ERRORS_PATH = \"../data/errors\"\n",
    "\n",
    "# Se crearon directorios si no existían\n",
    "Path(DATA_PROCESSED_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(ERRORS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Se verificó existencia del archivo\n",
    "if not Path(DATA_RAW_PATH).exists():\n",
    "    print(f\"ERROR: Archivo {DATA_RAW_PATH} no encontrado\")\n",
    "    print(\"Se requiere verificar que el archivo credit_risk_dataset.csv esté en data/raw/\")\n",
    "    print(\"El proceso ETL se pausará hasta que el archivo esté disponible\")\n",
    "else:\n",
    "    print(f\"Archivo encontrado: {DATA_RAW_PATH}\")\n",
    "    print(\"Se verificó la presencia del dataset de riesgo crediticio\")\n",
    "\n",
    "# Verificar tamaño del archivo si existe\n",
    "if Path(DATA_RAW_PATH).exists():\n",
    "    file_size = Path(DATA_RAW_PATH).stat().st_size\n",
    "    print(f\"Tamaño del archivo: {file_size / 1024:.1f} KB\")\n",
    "    log_operation(f\"Archivo verificado: {DATA_RAW_PATH} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab187154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-15 15:30:26] INFO: Se cargaron datos exitosamente: 5000 filas\n",
      "Dataset cargado:\n",
      "   - Filas: 5,000\n",
      "   - Columnas: 12\n",
      "   - Memoria estimada: 1571.4 KB\n",
      "\n",
      "Primeras 3 filas:\n",
      "  Fila 1: {'person_age': 58, 'person_income': 49184, 'person_home_ownership': 'RENT', 'person_emp_length': 17, 'loan_intent': 'EDUCATION', 'loan_grade': 'B', 'loan_amnt': 10144, 'loan_int_rate': 18.53, 'loan_status': 'Default', 'loan_percent_income': 0.498, 'cb_person_default_on_file': 'Y', 'cb_person_cred_hist_length': 1}\n",
      "  Fila 2: {'person_age': 23, 'person_income': 77314, 'person_home_ownership': 'OWN', 'person_emp_length': 32, 'loan_intent': 'HOMEIMPROVEMENT', 'loan_grade': 'A', 'loan_amnt': 37781, 'loan_int_rate': 8.96, 'loan_status': 'Non Default', 'loan_percent_income': 0.199, 'cb_person_default_on_file': 'N', 'cb_person_cred_hist_length': 26}\n",
      "  Fila 3: {'person_age': 73, 'person_income': 21703, 'person_home_ownership': 'OWN', 'person_emp_length': 27, 'loan_intent': 'MEDICAL', 'loan_grade': 'C', 'loan_amnt': 11189, 'loan_int_rate': 9.25, 'loan_status': 'Non Default', 'loan_percent_income': 0.103, 'cb_person_default_on_file': 'N', 'cb_person_cred_hist_length': 4}\n"
     ]
    }
   ],
   "source": [
    "# Función básica para cargar datos CSV\n",
    "def load_csv_data(file_path):\n",
    "    \"\"\"Carga datos CSV usando funciones básicas de Python\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            # Convertir tipos básicos\n",
    "            for key, value in row.items():\n",
    "                if value.isdigit():\n",
    "                    row[key] = int(value)\n",
    "                elif value.replace('.', '').isdigit():\n",
    "                    row[key] = float(value)\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "# Se cargaron datos\n",
    "try:\n",
    "    df_raw = load_csv_data(DATA_RAW_PATH)\n",
    "    log_operation(f\"Se cargaron datos exitosamente: {len(df_raw)} filas\")\n",
    "    \n",
    "    print(f\"Dataset cargado:\")\n",
    "    print(f\"   - Filas: {len(df_raw):,}\")\n",
    "    print(f\"   - Columnas: {len(df_raw[0].keys()) if df_raw else 0:,}\")\n",
    "    \n",
    "    # Calcular uso de memoria aproximado\n",
    "    memory_usage = len(str(df_raw))\n",
    "    print(f\"   - Memoria estimada: {memory_usage / 1024:.1f} KB\")\n",
    "    \n",
    "    # Mostrar primeras filas\n",
    "    print(f\"\\nPrimeras 3 filas:\")\n",
    "    for i, row in enumerate(df_raw[:3]):\n",
    "        print(f\"  Fila {i+1}: {row}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error cargando datos: {e}\")\n",
    "    log_operation(f\"Error en carga: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2506c",
   "metadata": {},
   "source": [
    "## 2. Exploración Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62cfae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFORMACIÓN GENERAL DEL DATASET\n",
      "==================================================\n",
      "Total de registros: 5,000\n",
      "Total de columnas: 12\n",
      "\n",
      "TIPOS DE DATOS:\n",
      "   person_age: int\n",
      "   person_income: int\n",
      "   person_home_ownership: string\n",
      "   person_emp_length: int\n",
      "   loan_intent: string\n",
      "   loan_grade: string\n",
      "   loan_amnt: int\n",
      "   loan_int_rate: float\n",
      "   loan_status: string\n",
      "   loan_percent_income: float\n",
      "   cb_person_default_on_file: string\n",
      "   cb_person_cred_hist_length: int\n",
      "\n",
      "PRIMERAS FILAS\n",
      "==================================================\n",
      "Fila 1:\n",
      "   person_age: 58\n",
      "   person_income: 49184\n",
      "   person_home_ownership: RENT\n",
      "   person_emp_length: 17\n",
      "   loan_intent: EDUCATION\n",
      "   loan_grade: B\n",
      "   loan_amnt: 10144\n",
      "   loan_int_rate: 18.53\n",
      "   loan_status: Default\n",
      "   loan_percent_income: 0.498\n",
      "   cb_person_default_on_file: Y\n",
      "   cb_person_cred_hist_length: 1\n",
      "\n",
      "Fila 2:\n",
      "   person_age: 23\n",
      "   person_income: 77314\n",
      "   person_home_ownership: OWN\n",
      "   person_emp_length: 32\n",
      "   loan_intent: HOMEIMPROVEMENT\n",
      "   loan_grade: A\n",
      "   loan_amnt: 37781\n",
      "   loan_int_rate: 8.96\n",
      "   loan_status: Non Default\n",
      "   loan_percent_income: 0.199\n",
      "   cb_person_default_on_file: N\n",
      "   cb_person_cred_hist_length: 26\n",
      "\n",
      "Fila 3:\n",
      "   person_age: 73\n",
      "   person_income: 21703\n",
      "   person_home_ownership: OWN\n",
      "   person_emp_length: 27\n",
      "   loan_intent: MEDICAL\n",
      "   loan_grade: C\n",
      "   loan_amnt: 11189\n",
      "   loan_int_rate: 9.25\n",
      "   loan_status: Non Default\n",
      "   loan_percent_income: 0.103\n",
      "   cb_person_default_on_file: N\n",
      "   cb_person_cred_hist_length: 4\n",
      "\n",
      "Fila 4:\n",
      "   person_age: 40\n",
      "   person_income: 110165\n",
      "   person_home_ownership: MORTGAGE\n",
      "   person_emp_length: 2\n",
      "   loan_intent: DEBTCONSOLIDATION\n",
      "   loan_grade: D\n",
      "   loan_amnt: 36142\n",
      "   loan_int_rate: 7.64\n",
      "   loan_status: Non Default\n",
      "   loan_percent_income: 0.084\n",
      "   cb_person_default_on_file: N\n",
      "   cb_person_cred_hist_length: 27\n",
      "\n",
      "Fila 5:\n",
      "   person_age: 58\n",
      "   person_income: 182141\n",
      "   person_home_ownership: MORTGAGE\n",
      "   person_emp_length: 36\n",
      "   loan_intent: EDUCATION\n",
      "   loan_grade: F\n",
      "   loan_amnt: 5558\n",
      "   loan_int_rate: 6.24\n",
      "   loan_status: Non Default\n",
      "   loan_percent_income: 0.646\n",
      "   cb_person_default_on_file: Y\n",
      "   cb_person_cred_hist_length: 28\n",
      "\n",
      "\n",
      "ESTADÍSTICAS DESCRIPTIVAS - Variables Numéricas\n",
      "==================================================\n",
      "person_age:\n",
      "   Min: 18\n",
      "   Max: 75\n",
      "   Promedio: 46.10\n",
      "   Valores únicos: 58\n",
      "\n",
      "person_income:\n",
      "   Min: 20092\n",
      "   Max: 199949\n",
      "   Promedio: 110981.30\n",
      "   Valores únicos: 4929\n",
      "\n",
      "person_emp_length:\n",
      "   Min: 0\n",
      "   Max: 40\n",
      "   Promedio: 19.88\n",
      "   Valores únicos: 41\n",
      "\n",
      "loan_amnt:\n",
      "   Min: 1003\n",
      "   Max: 39991\n",
      "   Promedio: 20469.29\n",
      "   Valores únicos: 4703\n",
      "\n",
      "loan_int_rate:\n",
      "   Min: 5.42\n",
      "   Max: 23.22\n",
      "   Promedio: 14.28\n",
      "   Valores únicos: 1682\n",
      "\n",
      "loan_percent_income:\n",
      "   Min: 0.021\n",
      "   Max: 0.829\n",
      "   Promedio: 0.42\n",
      "   Valores únicos: 805\n",
      "\n",
      "cb_person_cred_hist_length:\n",
      "   Min: 1\n",
      "   Max: 30\n",
      "   Promedio: 15.36\n",
      "   Valores únicos: 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análisis básico del dataset usando funciones nativas\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Información básica\n",
    "print(f\"Total de registros: {len(df_raw):,}\")\n",
    "print(f\"Total de columnas: {len(df_raw[0].keys()) if df_raw else 0:,}\")\n",
    "\n",
    "# Tipos de datos por columna\n",
    "print(f\"\\nTIPOS DE DATOS:\")\n",
    "if df_raw:\n",
    "    for col in df_raw[0].keys():\n",
    "        sample_values = [row[col] for row in df_raw[:100] if row[col] is not None]\n",
    "        if sample_values:\n",
    "            sample_val = sample_values[0]\n",
    "            if isinstance(sample_val, int):\n",
    "                data_type = \"int\"\n",
    "            elif isinstance(sample_val, float):\n",
    "                data_type = \"float\"\n",
    "            else:\n",
    "                data_type = \"string\"\n",
    "            print(f\"   {col}: {data_type}\")\n",
    "\n",
    "print(f\"\\nPRIMERAS FILAS\")\n",
    "print(\"=\" * 50)\n",
    "for i, row in enumerate(df_raw[:5]):\n",
    "    print(f\"Fila {i+1}:\")\n",
    "    for key, value in row.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "# Estadísticas básicas de variables numéricas\n",
    "print(f\"\\nESTADÍSTICAS DESCRIPTIVAS - Variables Numéricas\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "numeric_cols = []\n",
    "for col in df_raw[0].keys():\n",
    "    sample_val = df_raw[0][col]\n",
    "    if isinstance(sample_val, (int, float)):\n",
    "        numeric_cols.append(col)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    values = [row[col] for row in df_raw if isinstance(row[col], (int, float))]\n",
    "    if values:\n",
    "        mean_val = sum(values) / len(values)\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"   Min: {min_val}\")\n",
    "        print(f\"   Max: {max_val}\")\n",
    "        print(f\"   Promedio: {mean_val:.2f}\")\n",
    "        print(f\"   Valores únicos: {len(set(values))}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "224cb9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANÁLISIS DE VALORES FALTANTES\n",
      "==================================================\n",
      "✓ No se encontraron valores faltantes en el dataset\n",
      "[2025-10-15 15:30:35] INFO: Análisis de valores faltantes completado. Columnas críticas: 0\n"
     ]
    }
   ],
   "source": [
    "# Análisis de valores faltantes\n",
    "print(\"ANÁLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_stats = {}\n",
    "total_rows = len(df_raw)\n",
    "\n",
    "for col in df_raw[0].keys():\n",
    "    missing_count = sum(1 for row in df_raw if row[col] is None or row[col] == '' or row[col] == 'NULL')\n",
    "    missing_percentage = (missing_count / total_rows) * 100\n",
    "    missing_stats[col] = {\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percentage': missing_percentage\n",
    "    }\n",
    "\n",
    "# Mostrar estadísticas de valores faltantes\n",
    "has_missing = False\n",
    "for col, stats in missing_stats.items():\n",
    "    if stats['Missing_Count'] > 0:\n",
    "        has_missing = True\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"   Valores faltantes: {stats['Missing_Count']:,}\")\n",
    "        print(f\"   Porcentaje: {stats['Missing_Percentage']:.2f}%\")\n",
    "\n",
    "if not has_missing:\n",
    "    print(\"✓ No se encontraron valores faltantes en el dataset\")\n",
    "\n",
    "# Identificar columnas críticas con muchos faltantes\n",
    "critical_cols = [col for col, stats in missing_stats.items() if stats['Missing_Percentage'] > 50]\n",
    "if critical_cols:\n",
    "    print(f\"\\nCOLUMNAS CRÍTICAS CON >50% FALTANTES:\")\n",
    "    for col in critical_cols:\n",
    "        print(f\"   - {col}: {missing_stats[col]['Missing_Percentage']:.1f}%\")\n",
    "\n",
    "log_operation(f\"Análisis de valores faltantes completado. Columnas críticas: {len(critical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9c362",
   "metadata": {},
   "source": [
    "## 3. Identificación y Mapeo de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c760243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTIFICACIÓN DE VARIABLES CLAVE\n",
      "==================================================\n",
      "Columnas disponibles en el dataset:\n",
      " 1. person_age\n",
      " 2. person_income\n",
      " 3. person_home_ownership\n",
      " 4. person_emp_length\n",
      " 5. loan_intent\n",
      " 6. loan_grade\n",
      " 7. loan_amnt\n",
      " 8. loan_int_rate\n",
      " 9. loan_status\n",
      "10. loan_percent_income\n",
      "11. cb_person_default_on_file\n",
      "12. cb_person_cred_hist_length\n",
      "\n",
      "Total de columnas: 12\n",
      "\n",
      "Columnas relacionadas con riesgo encontradas:\n",
      "   - loan_intent\n",
      "     Valores únicos: ['DEBTCONSOLIDATION', 'PERSONAL', 'EDUCATION', 'MEDICAL', 'HOMEIMPROVEMENT', 'VENTURE']\n",
      "   - loan_grade\n",
      "     Valores únicos: ['D', 'G', 'A', 'F', 'B', 'E', 'C']\n",
      "   - loan_amnt\n",
      "     Rango: 1003 - 39991\n",
      "   - loan_int_rate\n",
      "     Rango: 5.42 - 23.22\n",
      "   - loan_status\n",
      "     Valores únicos: ['Non Default', 'Default']\n",
      "   - loan_percent_income\n",
      "     Rango: 0.021 - 0.829\n",
      "   - cb_person_default_on_file\n",
      "     Valores únicos: ['N', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# Se identificaron posibles variables target y features clave\n",
    "print(\"IDENTIFICACIÓN DE VARIABLES CLAVE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Se listaron todas las columnas para análisis\n",
    "print(\"Columnas disponibles en el dataset:\")\n",
    "columns = list(df_raw[0].keys())\n",
    "for i, col in enumerate(columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal de columnas: {len(columns)}\")\n",
    "\n",
    "# Se buscaron variables relacionadas con default o riesgo\n",
    "risk_related_cols = [col for col in columns if any(keyword in col.lower() \n",
    "                    for keyword in ['default', 'status', 'class', 'target', 'risk', 'loan'])]\n",
    "\n",
    "if risk_related_cols:\n",
    "    print(f\"\\nColumnas relacionadas con riesgo encontradas:\")\n",
    "    for col in risk_related_cols:\n",
    "        print(f\"   - {col}\")\n",
    "        # Obtener valores de la columna\n",
    "        col_values = [row[col] for row in df_raw if col in row]\n",
    "        # Determinar tipo de dato\n",
    "        if isinstance(col_values[0], str):\n",
    "            unique_vals = list(set(col_values))\n",
    "            print(f\"     Valores únicos: {unique_vals[:10]}\")\n",
    "        else:\n",
    "            print(f\"     Rango: {min(col_values)} - {max(col_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598721d",
   "metadata": {},
   "source": [
    "## 4. Validación y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccb65bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTIFICACIÓN DE VARIABLE TARGET\n",
      "==================================================\n",
      "Se encontraron candidatos para variable target:\n",
      "\n",
      "loan_status:\n",
      "   Tipo: str\n",
      "   Valores únicos: 2\n",
      "   Distribución: {'Non Default': 3762, 'Default': 1238}\n",
      "\n",
      "cb_person_default_on_file:\n",
      "   Tipo: str\n",
      "   Valores únicos: 2\n",
      "   Distribución: {'N': 3762, 'Y': 1238}\n"
     ]
    }
   ],
   "source": [
    "# Definir clase DataCleaning con métodos mínimos de limpieza y outliers\n",
    "class DataCleaning:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def handle_missing_values(self, df, strategy=None):\n",
    "        \"\"\"\n",
    "        Limpiar valores faltantes según estrategia:\n",
    "        - drop_threshold: eliminar columnas con % de nulos > umbral (0-1)\n",
    "        - numerical: 'median' o 'mean' para imputación\n",
    "        - categorical: 'mode' para imputación\n",
    "        Retorna un DataFrame limpio.\n",
    "        \"\"\"\n",
    "        import pandas as pd  # asegurar disponibilidad dentro del método\n",
    "\n",
    "        if df is None or len(df) == 0:\n",
    "            return df\n",
    "\n",
    "        if strategy is None:\n",
    "            strategy = {\n",
    "                'numerical': 'median',\n",
    "                'categorical': 'mode',\n",
    "                'drop_threshold': 0.7\n",
    "            }\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        # 1) Eliminar columnas con alto porcentaje de nulos\n",
    "        drop_threshold = float(strategy.get('drop_threshold', 0.7))\n",
    "        if 0 < drop_threshold < 1:\n",
    "            null_ratio = df.isnull().mean()\n",
    "            cols_to_drop = null_ratio[null_ratio > drop_threshold].index.tolist()\n",
    "            if cols_to_drop:\n",
    "                df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "        # 2) Imputación numérica\n",
    "        num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        if num_cols:\n",
    "            if strategy.get('numerical', 'median') == 'mean':\n",
    "                df[num_cols] = df[num_cols].fillna(df[num_cols].mean())\n",
    "            else:\n",
    "                df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "        # 3) Imputación categórica\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        for col in cat_cols:\n",
    "            if df[col].isnull().any():\n",
    "                mode_val = df[col].mode(dropna=True)\n",
    "                fill_val = mode_val.iloc[0] if not mode_val.empty else 'Desconocido'\n",
    "                df[col] = df[col].fillna(fill_val)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def handle_outliers(self, df, columns=None, method='iqr', threshold=1.5):\n",
    "        \"\"\"\n",
    "        Detectar outliers y retornar (df_sin_outliers, df_outliers) usando método IQR.\n",
    "        - columns: lista de columnas numéricas a evaluar; si None, detecta automáticamente.\n",
    "        - threshold: multiplicador del IQR (típicamente 1.5-3.0; financiero puede usar 2.0)\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        if df is None or len(df) == 0:\n",
    "            return df, pd.DataFrame(columns=df.columns if hasattr(df, 'columns') else [])\n",
    "\n",
    "        df = df.copy()\n",
    "        if columns is None:\n",
    "            columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Construir máscara de outliers agregada\n",
    "        outlier_mask = pd.Series(False, index=df.index)\n",
    "        for col in columns:\n",
    "            series = df[col].dropna()\n",
    "            if series.empty:\n",
    "                continue\n",
    "            q1 = series.quantile(0.25)\n",
    "            q3 = series.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            # Evitar división por cero\n",
    "            if iqr == 0:\n",
    "                continue\n",
    "            lower = q1 - threshold * iqr\n",
    "            upper = q3 + threshold * iqr\n",
    "            outlier_mask |= (df[col] < lower) | (df[col] > upper)\n",
    "\n",
    "        df_outliers = df[outlier_mask].copy()\n",
    "        df_no_outliers = df[~outlier_mask].copy()\n",
    "        return df_no_outliers, df_outliers\n",
    "\n",
    "    def create_derived_features(self, data):\n",
    "        \"\"\"\n",
    "        Crear features derivadas sobre lista de registros (list[dict]) o DataFrame.\n",
    "        Nuevas columnas:\n",
    "        - interest_monthly_rate\n",
    "        - installment_36m, installment_60m (PMT aproximado)\n",
    "        - risk_bucket (Low/Medium/High) con fallback si falta risk_score\n",
    "        - dti_bucket (<=0.2, 0.2-0.4, 0.4-0.6, >0.6) con fallback si falta debt_to_income_ratio\n",
    "        - grade_numeric (A=7..G=1)\n",
    "        - emp_length_bucket (0-1, 2-5, 6-10, >10)\n",
    "        - loan_to_income_ratio (si falta)\n",
    "        \"\"\"\n",
    "        # Utilidades\n",
    "        def _is_num(x):\n",
    "            return isinstance(x, (int, float))\n",
    "\n",
    "        def _pmt(rate, nper, pv):\n",
    "            # rate en unidad (p.ej. 0.01 mensual), nper meses, pv principal\n",
    "            if rate == 0:\n",
    "                return pv / nper if nper else 0.0\n",
    "            return rate * pv / (1 - (1 + rate) ** (-nper))\n",
    "\n",
    "        # Fallbacks para cálculos si faltan features previas\n",
    "        def _compute_risk_score(row):\n",
    "            # Misma regla del pipeline inicial\n",
    "            default_bonus = 30 if row.get('cb_person_default_on_file') == 'Y' else 0\n",
    "            grade = row.get('loan_grade')\n",
    "            try:\n",
    "                grade_part = (7 - ord(grade) + ord('A')) * 10 if isinstance(grade, str) and len(grade) == 1 else 0\n",
    "            except Exception:\n",
    "                grade_part = 0\n",
    "            ir = row.get('loan_int_rate')\n",
    "            ir_part = min((ir / 23.22) * 40, 40) if _is_num(ir) else 0\n",
    "            return default_bonus + grade_part + ir_part\n",
    "\n",
    "        def _risk_bucket(score):\n",
    "            if score is None:\n",
    "                return 'Desconocido'\n",
    "            if score < 40:\n",
    "                return 'Low'\n",
    "            if score < 70:\n",
    "                return 'Medium'\n",
    "            return 'High'\n",
    "\n",
    "        def _dti_from_row(row):\n",
    "            # Usar debt_to_income_ratio si existe, si no usar loan_percent_income\n",
    "            dti = row.get('debt_to_income_ratio')\n",
    "            if _is_num(dti):\n",
    "                return dti\n",
    "            lpi = row.get('loan_percent_income')\n",
    "            if _is_num(lpi):\n",
    "                return lpi\n",
    "            return None\n",
    "\n",
    "        def _dti_bucket(dti):\n",
    "            if dti is None:\n",
    "                return 'Desconocido'\n",
    "            if dti <= 0.2:\n",
    "                return '<=0.2'\n",
    "            if dti <= 0.4:\n",
    "                return '0.2-0.4'\n",
    "            if dti <= 0.6:\n",
    "                return '0.4-0.6'\n",
    "            return '>0.6'\n",
    "\n",
    "        grade_map = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\n",
    "\n",
    "        # Soportar DataFrame o lista de dicts\n",
    "        is_dataframe = False\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            is_dataframe = hasattr(data, 'to_dict') and isinstance(data, pd.DataFrame)\n",
    "        except Exception:\n",
    "            is_dataframe = False\n",
    "\n",
    "        if is_dataframe:\n",
    "            records = data.to_dict(orient='records')\n",
    "        else:\n",
    "            records = data\n",
    "\n",
    "        if not isinstance(records, list) or not records:\n",
    "            return data\n",
    "\n",
    "        # Crear features\n",
    "        out = []\n",
    "        for row in records:\n",
    "            r = dict(row)  # copia\n",
    "            # interest_monthly_rate\n",
    "            rate_m = None\n",
    "            if _is_num(r.get('loan_int_rate')):\n",
    "                rate_m = (r['loan_int_rate'] / 100.0) / 12.0\n",
    "                r['interest_monthly_rate'] = rate_m\n",
    "            else:\n",
    "                r['interest_monthly_rate'] = None\n",
    "\n",
    "            # loan_to_income_ratio (si no existe)\n",
    "            if 'loan_to_income_ratio' not in r:\n",
    "                if _is_num(r.get('loan_amnt')) and _is_num(r.get('person_income')) and r['person_income'] != 0:\n",
    "                    r['loan_to_income_ratio'] = r['loan_amnt'] / r['person_income']\n",
    "                else:\n",
    "                    r['loan_to_income_ratio'] = None\n",
    "\n",
    "            # installment estimado a 36 y 60 meses\n",
    "            if rate_m is not None and _is_num(r.get('loan_amnt')):\n",
    "                r['installment_36m'] = _pmt(rate_m, 36, r['loan_amnt'])\n",
    "                r['installment_60m'] = _pmt(rate_m, 60, r['loan_amnt'])\n",
    "            else:\n",
    "                r['installment_36m'] = None\n",
    "                r['installment_60m'] = None\n",
    "\n",
    "            # risk_score (fallback si falta)\n",
    "            rs = r.get('risk_score')\n",
    "            if not _is_num(rs):\n",
    "                rs = _compute_risk_score(r)\n",
    "                r['risk_score'] = rs\n",
    "\n",
    "            # risk_bucket en base a risk_score\n",
    "            r['risk_bucket'] = _risk_bucket(rs) if _is_num(rs) else 'Desconocido'\n",
    "\n",
    "            # debt_to_income_ratio (fallback a loan_percent_income si falta)\n",
    "            dti_val = _dti_from_row(r)\n",
    "            r['debt_to_income_ratio'] = dti_val if dti_val is not None else r.get('debt_to_income_ratio')\n",
    "\n",
    "            # dti_bucket\n",
    "            r['dti_bucket'] = _dti_bucket(dti_val)\n",
    "\n",
    "            # grade_numeric\n",
    "            r['grade_numeric'] = grade_map.get(r.get('loan_grade'), None)\n",
    "\n",
    "            # emp_length_bucket\n",
    "            emp_len = r.get('person_emp_length')\n",
    "            if _is_num(emp_len):\n",
    "                if emp_len <= 1:\n",
    "                    r['emp_length_bucket'] = '0-1'\n",
    "                elif emp_len <= 5:\n",
    "                    r['emp_length_bucket'] = '2-5'\n",
    "                elif emp_len <= 10:\n",
    "                    r['emp_length_bucket'] = '6-10'\n",
    "                else:\n",
    "                    r['emp_length_bucket'] = '>10'\n",
    "            else:\n",
    "                r['emp_length_bucket'] = 'Desconocido'\n",
    "\n",
    "            out.append(r)\n",
    "\n",
    "        if is_dataframe:\n",
    "            try:\n",
    "                import pandas as pd\n",
    "                return pd.DataFrame(out)\n",
    "            except Exception:\n",
    "                return out\n",
    "        return out\n",
    "\n",
    "# Se inicializó limpiador de datos\n",
    "cleaner = DataCleaning()\n",
    "\n",
    "# Se identificó la variable target según el dataset\n",
    "if len(df_raw) > 0:\n",
    "    columns = list(df_raw[0].keys())\n",
    "else:\n",
    "    columns = []\n",
    "\n",
    "target_candidates = [col for col in columns if any(keyword in col.lower() \n",
    "                    for keyword in ['default', 'status', 'class', 'target'])]\n",
    "\n",
    "print(\"IDENTIFICACIÓN DE VARIABLE TARGET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if target_candidates:\n",
    "    print(\"Se encontraron candidatos para variable target:\")\n",
    "    for col in target_candidates:\n",
    "        print(f\"\\n{col}:\")\n",
    "        # Obtener todos los valores de la columna\n",
    "        values = [row[col] for row in df_raw if col in row]\n",
    "        # Determinar tipo de dato\n",
    "        if values:\n",
    "            tipo = type(values[0]).__name__\n",
    "            print(f\"   Tipo: {tipo}\")\n",
    "            unique_vals = set(values)\n",
    "            print(f\"   Valores únicos: {len(unique_vals)}\")\n",
    "            if tipo == 'str' or len(unique_vals) < 20:\n",
    "                # Calcular distribución\n",
    "                from collections import Counter\n",
    "                dist = Counter(values)\n",
    "                print(f\"   Distribución: {dict(dist.most_common(5))}\")\n",
    "else:\n",
    "    print(\"No se encontraron variables target obvias\")\n",
    "    print(\"Se procederá con análisis exploratorio completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "924bae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREACIÓN DE VARIABLE DEFAULT Y PROCESAMIENTO\n",
      "==================================================\n",
      "Se encontró columna loan_status\n",
      "Valores únicos en loan_status:\n",
      "   Default: 1,238 (24.8%)\n",
      "   Non Default: 3,762 (75.2%)\n",
      "\n",
      "Variable target creada: default_flag\n",
      "Distribución de default_flag:\n",
      "   No Default: 3,762 (75.2%)\n",
      "   Default: 1,238 (24.8%)\n",
      "Tasa de default: 24.76%\n",
      "\n",
      "Variables adicionales creadas:\n",
      "   ✓ lgd_estimate\n",
      "   ✓ ead_amount\n",
      "   ✓ debt_to_income_ratio\n",
      "   ✓ risk_score\n",
      "   ✓ age_group\n",
      "   ✓ income_bracket\n",
      "[2025-10-15 16:12:33] INFO: Se creó variable target: default_flag (tasa default: 24.76%)\n",
      "\n",
      "✓ Total de registros procesados: 5,000\n",
      "✓ Total de columnas: 19\n"
     ]
    }
   ],
   "source": [
    "# Crear variable de default y procesar datos\n",
    "print(\"CREACIÓN DE VARIABLE DEFAULT Y PROCESAMIENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Se procesó la variable loan_status para crear default_flag\n",
    "print(\"Se encontró columna loan_status\")\n",
    "print(\"Valores únicos en loan_status:\")\n",
    "status_values = {}\n",
    "for row in df_raw:\n",
    "    status = row['loan_status']\n",
    "    status_values[status] = status_values.get(status, 0) + 1\n",
    "\n",
    "for status, count in status_values.items():\n",
    "    print(f\"   {status}: {count:,} ({count/len(df_raw)*100:.1f}%)\")\n",
    "\n",
    "# Crear variable default_flag\n",
    "df_clean = []\n",
    "for row in df_raw.copy():\n",
    "    # Crear nueva fila con todas las columnas originales\n",
    "    new_row = row.copy()\n",
    "    \n",
    "    # Crear variable target default_flag\n",
    "    new_row['default_flag'] = 1 if row['loan_status'] == 'Default' else 0\n",
    "    \n",
    "    # Crear variables adicionales para LGD y EAD\n",
    "    # LGD (Loss Given Default) - simulado basado en loan_grade\n",
    "    grade_lgd_map = {'A': 0.1, 'B': 0.2, 'C': 0.3, 'D': 0.4, 'E': 0.5, 'F': 0.6, 'G': 0.7}\n",
    "    new_row['lgd_estimate'] = grade_lgd_map.get(row['loan_grade'], 0.4)\n",
    "    \n",
    "    # EAD (Exposure at Default) - usando loan_amnt como proxy\n",
    "    new_row['ead_amount'] = row['loan_amnt']\n",
    "    \n",
    "    # Variables derivadas para dashboard\n",
    "    new_row['debt_to_income_ratio'] = row['loan_percent_income']\n",
    "    new_row['risk_score'] = (\n",
    "        (1 if row['cb_person_default_on_file'] == 'Y' else 0) * 30 +\n",
    "        (7 - ord(row['loan_grade']) + ord('A')) * 10 +\n",
    "        min(row['loan_int_rate'] / 23.22 * 40, 40)\n",
    "    )\n",
    "    new_row['age_group'] = (\n",
    "        '18-25' if row['person_age'] <= 25 else\n",
    "        '26-35' if row['person_age'] <= 35 else\n",
    "        '36-50' if row['person_age'] <= 50 else\n",
    "        '51-65' if row['person_age'] <= 65 else\n",
    "        '65+'\n",
    "    )\n",
    "    new_row['income_bracket'] = (\n",
    "        'Low' if row['person_income'] <= 50000 else\n",
    "        'Medium' if row['person_income'] <= 100000 else\n",
    "        'High'\n",
    "    )\n",
    "    \n",
    "    df_clean.append(new_row)\n",
    "\n",
    "# Verificar la creación de variables\n",
    "target_col = 'default_flag'\n",
    "print(f\"\\nVariable target creada: {target_col}\")\n",
    "\n",
    "# Contar defaults\n",
    "default_counts = {'No Default': 0, 'Default': 0}\n",
    "for row in df_clean:\n",
    "    if row['default_flag'] == 1:\n",
    "        default_counts['Default'] += 1\n",
    "    else:\n",
    "        default_counts['No Default'] += 1\n",
    "\n",
    "print(f\"Distribución de default_flag:\")\n",
    "for status, count in default_counts.items():\n",
    "    print(f\"   {status}: {count:,} ({count/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "default_rate = default_counts['Default'] / len(df_clean)\n",
    "print(f\"Tasa de default: {default_rate:.2%}\")\n",
    "\n",
    "# Mostrar variables adicionales creadas\n",
    "print(f\"\\nVariables adicionales creadas:\")\n",
    "new_vars = ['lgd_estimate', 'ead_amount', 'debt_to_income_ratio', 'risk_score', 'age_group', 'income_bracket']\n",
    "for var in new_vars:\n",
    "    print(f\"   ✓ {var}\")\n",
    "\n",
    "log_operation(f\"Se creó variable target: {target_col} (tasa default: {default_rate:.2%})\")\n",
    "print(f\"\\n✓ Total de registros procesados: {len(df_clean):,}\")\n",
    "print(f\"✓ Total de columnas: {len(df_clean[0].keys()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fcc4616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MANEJO DE VALORES FALTANTES (DataCleaning + fallback)\n",
      "==================================================\n",
      "No se pudo usar pandas (No module named 'pandas'). Se activa fallback sin pandas.\n",
      "Limpieza completada (fallback):\n",
      "   - Filas restantes: 5,000\n",
      "   - Columnas restantes: 19\n",
      "   - Valores faltantes restantes: 0\n",
      "[2025-10-15 16:16:32] INFO: Se completó manejo de faltantes. Filas finales: 5000\n"
     ]
    }
   ],
   "source": [
    "# Manejo de valores faltantes usando DataCleaning (pandas) con fallback a implementación sin pandas\n",
    "print(\"MANEJO DE VALORES FALTANTES (DataCleaning + fallback)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Estrategia desde ETL_CONFIG\n",
    "missing_strategy = ETL_CONFIG['missing_values'] if 'ETL_CONFIG' in globals() and 'missing_values' in ETL_CONFIG else {\n",
    "    'drop_threshold': 0.7,\n",
    "    'numerical': 'median',\n",
    "    'categorical': 'mode'\n",
    "}\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    # Convertir a DataFrame si es lista de dicts\n",
    "    if isinstance(df_clean, list):\n",
    "        df_clean_df = pd.DataFrame(df_clean)\n",
    "    else:\n",
    "        df_clean_df = df_clean  # asumir que ya es DataFrame\n",
    "\n",
    "    original_rows = len(df_clean_df)\n",
    "    original_cols = len(df_clean_df.columns) if hasattr(df_clean_df, 'columns') else 0\n",
    "\n",
    "    # Guardar filas con nulos antes de limpiar\n",
    "    pre_missing = df_clean_df[df_clean_df.isna().any(axis=1)] if original_rows > 0 else pd.DataFrame()\n",
    "    if not pre_missing.empty:\n",
    "        Path(ERRORS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "        pre_missing.to_csv(f\"{ERRORS_PATH}/initial_missing_values_rows.csv\", index=False, encoding='utf-8')\n",
    "        print(f\"Se guardaron {len(pre_missing):,} filas con valores faltantes en data/errors/\")\n",
    "\n",
    "    # Aplicar limpieza mediante DataCleaning\n",
    "    df_clean_df = cleaner.handle_missing_values(df_clean_df, strategy=missing_strategy)\n",
    "\n",
    "    # Métricas post-limpieza\n",
    "    remaining_missing = int(df_clean_df.isna().sum().sum())\n",
    "    final_rows = len(df_clean_df)\n",
    "    final_cols = len(df_clean_df.columns)\n",
    "\n",
    "    print(\"Limpieza completada (pandas):\")\n",
    "    print(f\"   - Filas originales: {original_rows:,}\")\n",
    "    print(f\"   - Columnas originales: {original_cols:,}\")\n",
    "    print(f\"   - Filas restantes: {final_rows:,}\")\n",
    "    print(f\"   - Columnas restantes: {final_cols:,}\")\n",
    "    print(f\"   - Valores faltantes restantes: {remaining_missing:,}\")\n",
    "\n",
    "    # Volver a lista de dicts para compatibilidad con celdas siguientes\n",
    "    df_clean = df_clean_df.to_dict(orient='records')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo usar pandas ({e}). Se activa fallback sin pandas.\")\n",
    "\n",
    "    from collections import Counter\n",
    "    from statistics import median, mean\n",
    "    import csv as _csv\n",
    "\n",
    "    def _is_missing(val):\n",
    "        return val is None or val == '' or val == 'NULL'\n",
    "\n",
    "    def _write_rows_csv(rows, filename):\n",
    "        if not rows:\n",
    "            return\n",
    "        headers = set()\n",
    "        for r in rows:\n",
    "            headers.update(r.keys())\n",
    "        headers = list(headers)\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = _csv.DictWriter(f, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            for r in rows:\n",
    "                writer.writerow(r)\n",
    "\n",
    "    if not isinstance(df_clean, list) or len(df_clean) == 0:\n",
    "        print(\"No hay datos en df_clean para limpiar.\")\n",
    "    else:\n",
    "        total_rows = len(df_clean)\n",
    "        all_cols = set()\n",
    "        for row in df_clean:\n",
    "            all_cols.update(row.keys())\n",
    "        all_cols = list(all_cols)\n",
    "\n",
    "        missing_counts = {}\n",
    "        for col in all_cols:\n",
    "            miss = 0\n",
    "            for row in df_clean:\n",
    "                val = row.get(col, None)\n",
    "                if _is_missing(val):\n",
    "                    miss += 1\n",
    "            missing_counts[col] = miss\n",
    "\n",
    "        problematic_rows = [r.copy() for r in df_clean if any(_is_missing(r.get(c, None)) for c in all_cols)]\n",
    "        if problematic_rows:\n",
    "            _write_rows_csv(problematic_rows, f\"{ERRORS_PATH}/initial_missing_values_rows.csv\")\n",
    "            print(f\"Se guardaron {len(problematic_rows)} filas con valores faltantes en data/errors/\")\n",
    "\n",
    "        drop_threshold = float(missing_strategy.get('drop_threshold', 0.7))\n",
    "        cols_to_drop = [c for c in all_cols if (missing_counts[c] / total_rows) > drop_threshold]\n",
    "        if cols_to_drop:\n",
    "            for row in df_clean:\n",
    "                for c in cols_to_drop:\n",
    "                    if c in row:\n",
    "                        row.pop(c)\n",
    "            print(f\"Columnas eliminadas por alto porcentaje de nulos: {cols_to_drop}\")\n",
    "\n",
    "        remaining_cols = set()\n",
    "        for row in df_clean:\n",
    "            remaining_cols.update(row.keys())\n",
    "        remaining_cols = list(remaining_cols)\n",
    "\n",
    "        def _is_numeric(value):\n",
    "            return isinstance(value, (int, float))\n",
    "\n",
    "        numeric_cols = []\n",
    "        categorical_cols = []\n",
    "        for col in remaining_cols:\n",
    "            non_missing_vals = [row.get(col, None) for row in df_clean if not _is_missing(row.get(col, None))]\n",
    "            if non_missing_vals and all(_is_numeric(v) for v in non_missing_vals):\n",
    "                numeric_cols.append(col)\n",
    "            else:\n",
    "                categorical_cols.append(col)\n",
    "\n",
    "        num_strategy = missing_strategy.get('numerical', 'median')\n",
    "        for col in numeric_cols:\n",
    "            non_missing_vals = [row.get(col) for row in df_clean if not _is_missing(row.get(col, None))]\n",
    "            if not non_missing_vals:\n",
    "                continue\n",
    "            fill_val = median(non_missing_vals) if num_strategy != 'mean' else mean(non_missing_vals)\n",
    "            for row in df_clean:\n",
    "                if _is_missing(row.get(col, None)):\n",
    "                    row[col] = fill_val\n",
    "\n",
    "        cat_strategy = missing_strategy.get('categorical', 'mode')\n",
    "        for col in categorical_cols:\n",
    "            non_missing_vals = [row.get(col) for row in df_clean if not _is_missing(row.get(col, None))]\n",
    "            fill_val = None\n",
    "            if non_missing_vals and cat_strategy == 'mode':\n",
    "                fill_val = Counter(non_missing_vals).most_common(1)[0][0]\n",
    "            if fill_val is None:\n",
    "                fill_val = 'Desconocido'\n",
    "            for row in df_clean:\n",
    "                if _is_missing(row.get(col, None)):\n",
    "                    row[col] = fill_val\n",
    "\n",
    "        remaining_missing = 0\n",
    "        for row in df_clean:\n",
    "            for c in row.keys():\n",
    "                if _is_missing(row.get(c, None)):\n",
    "                    remaining_missing += 1\n",
    "\n",
    "        final_cols = len(df_clean[0].keys()) if df_clean else 0\n",
    "        print(\"Limpieza completada (fallback):\")\n",
    "        print(f\"   - Filas restantes: {len(df_clean):,}\")\n",
    "        print(f\"   - Columnas restantes: {final_cols:,}\")\n",
    "        print(f\"   - Valores faltantes restantes: {remaining_missing:,}\")\n",
    "\n",
    "log_operation(f\"Se completó manejo de faltantes. Filas finales: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1d26a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECCIÓN DE OUTLIERS (DataCleaning + fallback)\n",
      "==================================================\n",
      "No se pudo usar pandas (No module named 'pandas'). Se activa fallback sin pandas.\n",
      "\n",
      "Análisis de outliers:\n",
      "   - Outliers detectados: 0 (0.0%)\n",
      "   - Datos sin outliers (solo referencia): 5,000\n",
      "Decisión: Se mantuvieron outliers documentados para análisis posterior\n",
      "[2025-10-15 16:17:33] INFO: Se detectaron y documentaron outliers: 0\n"
     ]
    }
   ],
   "source": [
    "# Detección de outliers usando DataCleaning (pandas) con fallback sin pandas\n",
    "print(\"DETECCIÓN DE OUTLIERS (DataCleaning + fallback)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuración desde ETL_CONFIG\n",
    "_out_cfg = ETL_CONFIG['outliers'] if 'ETL_CONFIG' in globals() and 'outliers' in ETL_CONFIG else {\n",
    "    'method': 'iqr',\n",
    "    'threshold': 2.0,\n",
    "    'exclude_columns': ['default_flag']\n",
    "}\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    # Asegurar DataFrame\n",
    "    df_clean_df = pd.DataFrame(df_clean) if isinstance(df_clean, list) else df_clean\n",
    "\n",
    "    if df_clean_df is None or df_clean_df.empty:\n",
    "        print(\"No hay datos para detectar outliers.\")\n",
    "        df_outliers = []\n",
    "        df_no_outliers = []\n",
    "        df_final = df_clean\n",
    "    else:\n",
    "        numeric_cols = df_clean_df.select_dtypes(include=['number']).columns.tolist()\n",
    "        numeric_cols = [c for c in numeric_cols if c not in _out_cfg.get('exclude_columns', [])]\n",
    "\n",
    "        df_no_outliers_df, df_outliers_df = cleaner.handle_outliers(\n",
    "            df_clean_df,\n",
    "            columns=numeric_cols,\n",
    "            method=_out_cfg.get('method', 'iqr'),\n",
    "            threshold=float(_out_cfg.get('threshold', 2.0))\n",
    "        )\n",
    "\n",
    "        if df_outliers_df is not None and not df_outliers_df.empty:\n",
    "            Path(ERRORS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "            df_outliers_df.to_csv(f\"{ERRORS_PATH}/outliers_detected_rows.csv\", index=False, encoding='utf-8')\n",
    "            print(f\"Se guardaron {len(df_outliers_df):,} outliers en data/errors/\")\n",
    "\n",
    "        df_outliers = df_outliers_df.to_dict(orient='records') if hasattr(df_outliers_df, 'to_dict') else []\n",
    "        df_no_outliers = df_no_outliers_df.to_dict(orient='records') if hasattr(df_no_outliers_df, 'to_dict') else []\n",
    "        df_final = df_clean_df.to_dict(orient='records')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo usar pandas ({e}). Se activa fallback sin pandas.\")\n",
    "\n",
    "    # Fallback: implementación sin pandas (usando IQR)\n",
    "    if not isinstance(df_clean, list) or len(df_clean) == 0:\n",
    "        print(\"No hay datos para detectar outliers.\")\n",
    "        df_outliers = []\n",
    "        df_no_outliers = []\n",
    "        df_final = df_clean\n",
    "    else:\n",
    "        def _is_numeric(value):\n",
    "            return isinstance(value, (int, float))\n",
    "\n",
    "        numeric_cols = []\n",
    "        for col in df_clean[0].keys():\n",
    "            if col in _out_cfg.get('exclude_columns', []):\n",
    "                continue\n",
    "            non_missing_vals = [row.get(col, None) for row in df_clean if row.get(col, None) is not None]\n",
    "            if non_missing_vals and all(_is_numeric(v) for v in non_missing_vals):\n",
    "                numeric_cols.append(col)\n",
    "\n",
    "        def _percentile(sorted_vals, p):\n",
    "            if not sorted_vals:\n",
    "                return None\n",
    "            k = (len(sorted_vals) - 1) * p\n",
    "            f = int(k)\n",
    "            c = min(f + 1, len(sorted_vals) - 1)\n",
    "            if f == c:\n",
    "                return sorted_vals[f]\n",
    "            d0 = sorted_vals[f] * (c - k)\n",
    "            d1 = sorted_vals[c] * (k - f)\n",
    "            return d0 + d1\n",
    "\n",
    "        outlier_indices = set()\n",
    "        thr = float(_out_cfg.get('threshold', 2.0))\n",
    "        for col in numeric_cols:\n",
    "            vals = [row[col] for row in df_clean if row.get(col, None) is not None]\n",
    "            if len(vals) < 4:\n",
    "                continue\n",
    "            vals_sorted = sorted(vals)\n",
    "            q1 = _percentile(vals_sorted, 0.25)\n",
    "            q3 = _percentile(vals_sorted, 0.75)\n",
    "            iqr = q3 - q1\n",
    "            if iqr == 0:\n",
    "                continue\n",
    "            lower = q1 - thr * iqr\n",
    "            upper = q3 + thr * iqr\n",
    "            for idx, row in enumerate(df_clean):\n",
    "                v = row.get(col, None)\n",
    "                if v is None:\n",
    "                    continue\n",
    "                if v < lower or v > upper:\n",
    "                    outlier_indices.add(idx)\n",
    "\n",
    "        df_outliers = [df_clean[i] for i in sorted(outlier_indices)]\n",
    "        df_no_outliers = [r for i, r in enumerate(df_clean) if i not in outlier_indices]\n",
    "\n",
    "        if df_outliers:\n",
    "            headers = set()\n",
    "            for r in df_outliers:\n",
    "                headers.update(r.keys())\n",
    "            headers = list(headers)\n",
    "            with open(f\"{ERRORS_PATH}/outliers_detected_rows.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "                import csv as _csv\n",
    "                writer = _csv.DictWriter(f, fieldnames=headers)\n",
    "                writer.writeheader()\n",
    "                for rr in df_outliers:\n",
    "                    writer.writerow(rr)\n",
    "            print(f\"Se guardaron {len(df_outliers)} outliers en data/errors/\")\n",
    "\n",
    "        df_final = df_clean\n",
    "\n",
    "print(f\"\\nAnálisis de outliers:\")\n",
    "print(f\"   - Outliers detectados: {len(df_outliers):,} ({(len(df_outliers)/len(df_final)) if df_final else 0:.1%})\")\n",
    "print(f\"   - Datos sin outliers (solo referencia): {len(df_no_outliers):,}\")\n",
    "print(\"Decisión: Se mantuvieron outliers documentados para análisis posterior\")\n",
    "\n",
    "log_operation(f\"Se detectaron y documentaron outliers: {len(df_outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479cb04",
   "metadata": {},
   "source": [
    "## 5. Ingeniería de Características Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f176ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREACIÓN DE FEATURES DERIVADAS\n",
      "==================================================\n",
      "Features derivadas creadas (8):\n",
      "   - interest_monthly_rate\n",
      "   - installment_36m\n",
      "   - installment_60m\n",
      "   - risk_bucket\n",
      "   - dti_bucket\n",
      "   - grade_numeric\n",
      "   - emp_length_bucket\n",
      "   - loan_to_income_ratio\n",
      "\n",
      "Estadísticas de features derivadas:\n",
      "   - loan_to_income_ratio_mean: 0.25793137920856746\n",
      "   - installment_36m_mean: 703.6174082025229\n",
      "   - installment_60m_mean: 481.1637702140932\n",
      "   - risk_bucket_dist: {'High': 2636, 'Medium': 1795, 'Low': 569}\n",
      "   - dti_bucket_dist: {'>0.6': 1386, '0.4-0.6': 1267, '0.2-0.4': 1233, '<=0.2': 1114}\n",
      "[2025-10-15 16:18:23] INFO: Se crearon features derivadas: 8\n"
     ]
    }
   ],
   "source": [
    "# Ingeniería de características sin pandas (usar lista de dicts)\n",
    "print(\"CREACIÓN DE FEATURES DERIVADAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Aplicar creación de features sobre df_final (o df_clean si es lista)\n",
    "try:\n",
    "    # Si df_final quedó como lista tras pasos previos\n",
    "    if isinstance(df_final, list):\n",
    "        df_final = cleaner.create_derived_features(df_final)\n",
    "    else:\n",
    "        # Si fuera DataFrame en algún entorno, también soportado\n",
    "        df_final = cleaner.create_derived_features(df_final)\n",
    "except Exception as e:\n",
    "    print(f\"Error creando features derivadas: {e}\")\n",
    "\n",
    "# Identificar nuevas features creadas por sufijos/prefijos definidos\n",
    "feature_candidates = ['interest_monthly_rate', 'installment_36m', 'installment_60m',\n",
    "                      'risk_bucket', 'dti_bucket', 'grade_numeric', 'emp_length_bucket',\n",
    "                      'loan_to_income_ratio']\n",
    "\n",
    "created = [c for c in feature_candidates if c in df_final[0]] if df_final else []\n",
    "print(f\"Features derivadas creadas ({len(created)}):\")\n",
    "for feature in created:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "# Métricas simples sin pandas\n",
    "def _mean_safe(vals):\n",
    "    vals = [v for v in vals if isinstance(v, (int, float))]\n",
    "    return (sum(vals)/len(vals)) if vals else None\n",
    "\n",
    "if df_final:\n",
    "    # Calcular estadísticas para algunas features\n",
    "    stats = {\n",
    "        'loan_to_income_ratio_mean': _mean_safe([r.get('loan_to_income_ratio') for r in df_final]),\n",
    "        'installment_36m_mean': _mean_safe([r.get('installment_36m') for r in df_final]),\n",
    "        'installment_60m_mean': _mean_safe([r.get('installment_60m') for r in df_final]),\n",
    "        'risk_bucket_dist': None,\n",
    "        'dti_bucket_dist': None\n",
    "    }\n",
    "\n",
    "    # Distribuciones categóricas\n",
    "    from collections import Counter as _Counter\n",
    "    stats['risk_bucket_dist'] = dict(_Counter([r.get('risk_bucket') for r in df_final]).most_common())\n",
    "    stats['dti_bucket_dist'] = dict(_Counter([r.get('dti_bucket') for r in df_final]).most_common())\n",
    "\n",
    "    print(\"\\nEstadísticas de features derivadas:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"   - {k}: {v}\")\n",
    "\n",
    "log_operation(f\"Se crearon features derivadas: {len(created)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72475b45",
   "metadata": {},
   "source": [
    "## 6. Validación Final y Exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfb16169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACIÓN FINAL DE CALIDAD\n",
      "==================================================\n",
      "Resumen final del dataset:\n",
      "   - Total Rows: 5,000\n",
      "   - Total Columns: 19\n",
      "   - Missing Values: 0\n",
      "   - Duplicate Rows: 0\n",
      "   - Memory Usage Mb: 2.36\n",
      "\n",
      "Validaciones específicas:\n",
      "   - Tasa de default: 24.76%\n",
      "   - Variables numéricas principales: 5\n",
      "     person_age: promedio=46.10, std=16.53\n",
      "     person_income: promedio=110981.30, std=51504.44\n",
      "     loan_amnt: promedio=20469.29, std=11266.81\n",
      "     loan_int_rate: promedio=14.28, std=5.09\n",
      "     risk_score: promedio=72.09, std=25.60\n",
      "   - Variables categóricas: 5\n",
      "     person_home_ownership: 4 valores únicos\n",
      "     loan_intent: 6 valores únicos\n",
      "     loan_grade: 7 valores únicos\n",
      "     age_group: 5 valores únicos\n",
      "     income_bracket: 3 valores únicos\n",
      "\n",
      "Validaciones para dashboard:\n",
      "   - Variables target PD/LGD/EAD: ✓ Creadas\n",
      "   - Variables segmentación: ✓ age_group, income_bracket\n",
      "   - Variables KPI: ✓ risk_score, debt_to_income_ratio\n",
      "   - Variables temporales: ✓ Listas para análisis de tendencias\n",
      "[2025-10-15 16:18:35] INFO: Se completó validación final | Data: {'total_rows': 5000, 'total_columns': 19, 'missing_values': 0, 'duplicate_rows': 0, 'memory_usage_mb': 2.358841896057129}\n",
      "\n",
      "✓ Validación completada exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Validación final de calidad de datos\n",
    "print(\"VALIDACIÓN FINAL DE CALIDAD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Usar df_clean como df_final\n",
    "df_final = df_clean\n",
    "\n",
    "# Verificar integridad de datos críticos\n",
    "validation_results = {\n",
    "    'total_rows': len(df_final),\n",
    "    'total_columns': len(df_final[0].keys()),\n",
    "    'missing_values': 0,  # Ya verificamos que no hay valores faltantes\n",
    "    'duplicate_rows': 0,  # Verificación básica de duplicados\n",
    "    'memory_usage_mb': len(str(df_final)) / (1024**2)\n",
    "}\n",
    "\n",
    "# Verificar duplicados básicos (basado en ID único si existe)\n",
    "# Para este caso, asumimos registros únicos por la generación sintética\n",
    "\n",
    "print(\"Resumen final del dataset:\")\n",
    "for key, value in validation_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   - {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   - {key.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "# Validaciones específicas de riesgo crediticio\n",
    "print(f\"\\nValidaciones específicas:\")\n",
    "\n",
    "# Variable target principal\n",
    "if any('default_flag' in row for row in df_final):\n",
    "    default_count = sum(1 for row in df_final if row['default_flag'] == 1)\n",
    "    default_rate = default_count / len(df_final)\n",
    "    print(f\"   - Tasa de default: {default_rate:.2%}\")\n",
    "    if default_rate < 0.05 or default_rate > 0.50:\n",
    "        print(f\"   NOTA: Tasa de default: {default_rate:.2%} (verificar si es realista)\")\n",
    "\n",
    "# Análisis de variables numéricas clave\n",
    "numeric_cols = ['person_age', 'person_income', 'loan_amnt', 'loan_int_rate', 'risk_score']\n",
    "print(f\"   - Variables numéricas principales: {len(numeric_cols)}\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_final[0]:\n",
    "        values = [row[col] for row in df_final if isinstance(row[col], (int, float))]\n",
    "        if values:\n",
    "            mean_val = sum(values) / len(values)\n",
    "            std_val = (sum((x - mean_val) ** 2 for x in values) / len(values)) ** 0.5\n",
    "            print(f\"     {col}: promedio={mean_val:.2f}, std={std_val:.2f}\")\n",
    "\n",
    "# Columnas categóricas\n",
    "categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'age_group', 'income_bracket']\n",
    "print(f\"   - Variables categóricas: {len(categorical_cols)}\")\n",
    "for col in categorical_cols:\n",
    "    if col in df_final[0]:\n",
    "        unique_values = set(row[col] for row in df_final)\n",
    "        print(f\"     {col}: {len(unique_values)} valores únicos\")\n",
    "\n",
    "# Validaciones para dashboard\n",
    "print(f\"\\nValidaciones para dashboard:\")\n",
    "print(f\"   - Variables target PD/LGD/EAD: ✓ Creadas\")\n",
    "print(f\"   - Variables segmentación: ✓ age_group, income_bracket\")\n",
    "print(f\"   - Variables KPI: ✓ risk_score, debt_to_income_ratio\")\n",
    "print(f\"   - Variables temporales: ✓ Listas para análisis de tendencias\")\n",
    "\n",
    "log_operation(\"Se completó validación final\", \"INFO\", validation_results)\n",
    "print(f\"\\n✓ Validación completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d077fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPORTACIÓN DE DATOS LIMPIOS\n",
      "==================================================\n",
      "Dataset exportado:\n",
      "   - Archivo principal: ../data/processed/clean_data.csv\n",
      "   - Archivo versionado: ../data/processed/clean_data_20251015_162904.csv\n",
      "   - Tamaño: 550.5 KB\n",
      "   QC: filas con qc_error = 0 de 4,863\n",
      "   Partición escrita: ../data/processed/dashboard_partitions/snapshot_date=2025-10-15.csv (registros: 4,863)\n",
      "   Aviso: Consolidado existente detectado - aplicando APPEND + DEDUP por (snapshot_date, loan_id)\n",
      "   Consolidado actualizado:\n",
      "      - Total histórico (únicos por fecha y préstamo): 4,863\n",
      "      - Snapshots únicos en archivo: 1\n",
      "   Recomendación: ejecutar ETL diariamente por 7-30 días para MTD/MoM/YoY\n",
      "[2025-10-15 16:29:05] INFO: Se exportó dataset limpio: ../data/processed/clean_data.csv\n",
      "[2025-10-15 16:29:05] INFO: Partición escrita: ../data/processed/dashboard_partitions/snapshot_date=2025-10-15.csv\n",
      "[2025-10-15 16:29:05] INFO: Consolidado actualizado: ../data/processed/dashboard_data.csv\n",
      "\n",
      "Exportación completada exitosamente\n",
      "Archivos listos para EDA y Dashboard\n",
      "   QC: filas con qc_error = 0 de 4,863\n",
      "   Partición escrita: ../data/processed/dashboard_partitions/snapshot_date=2025-10-15.csv (registros: 4,863)\n",
      "   Aviso: Consolidado existente detectado - aplicando APPEND + DEDUP por (snapshot_date, loan_id)\n",
      "   Consolidado actualizado:\n",
      "      - Total histórico (únicos por fecha y préstamo): 4,863\n",
      "      - Snapshots únicos en archivo: 1\n",
      "   Recomendación: ejecutar ETL diariamente por 7-30 días para MTD/MoM/YoY\n",
      "[2025-10-15 16:29:05] INFO: Se exportó dataset limpio: ../data/processed/clean_data.csv\n",
      "[2025-10-15 16:29:05] INFO: Partición escrita: ../data/processed/dashboard_partitions/snapshot_date=2025-10-15.csv\n",
      "[2025-10-15 16:29:05] INFO: Consolidado actualizado: ../data/processed/dashboard_data.csv\n",
      "\n",
      "Exportación completada exitosamente\n",
      "Archivos listos para EDA y Dashboard\n"
     ]
    }
   ],
   "source": [
    "# Exportar dataset limpio (snapshots diarios con partición por fecha y deduplicación)\n",
    "print(\"EXPORTACIÓN DE DATOS LIMPIOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import csv, math, re\n",
    "\n",
    "# Archivos de salida\n",
    "output_file = f\"{DATA_PROCESSED_PATH}/clean_data.csv\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_file = f\"{DATA_PROCESSED_PATH}/clean_data_{timestamp}.csv\"\n",
    "\n",
    "# Identificadores/auditoría\n",
    "batch_id = f\"ETL_{timestamp}\"\n",
    "data_version = \"1.0\"\n",
    "\n",
    "# Función para escribir CSV unificando cabeceras\n",
    "def write_csv_data(data, filename):\n",
    "    \"\"\"Escribe datos en formato CSV (lista de diccionarios).\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not data:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            f.write('')\n",
    "        return filename\n",
    "    headers = set()\n",
    "    for row in data:\n",
    "        headers.update(row.keys())\n",
    "    headers = list(headers)\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    return filename\n",
    "\n",
    "try:\n",
    "    # Exportar archivo principal y versionado\n",
    "    write_csv_data(df_final, output_file)\n",
    "    write_csv_data(df_final, versioned_file)\n",
    "    \n",
    "    print(\"Dataset exportado:\")\n",
    "    print(f\"   - Archivo principal: {output_file}\")\n",
    "    print(f\"   - Archivo versionado: {versioned_file}\")\n",
    "    \n",
    "    if Path(output_file).exists():\n",
    "        file_size = Path(output_file).stat().st_size\n",
    "        print(f\"   - Tamaño: {file_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # Crear dataset optimizado para dashboard\n",
    "    dashboard_file = f\"{DATA_PROCESSED_PATH}/dashboard_data.csv\"\n",
    "    snapshot_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Utilidad percentil (p en [0,1])\n",
    "    def _percentile(vals, p):\n",
    "        vals = [v for v in vals if isinstance(v, (int, float))]\n",
    "        if not vals:\n",
    "            return None\n",
    "        s = sorted(vals)\n",
    "        k = (len(s) - 1) * p\n",
    "        f = int(k)\n",
    "        c = min(f + 1, len(s) - 1)\n",
    "        if f == c:\n",
    "            return s[f]\n",
    "        d0 = s[f] * (c - k)\n",
    "        d1 = s[c] * (k - f)\n",
    "        return d0 + d1\n",
    "\n",
    "    # Percentil 30 de risk_score para regla de alto riesgo\n",
    "    risk_scores = [r.get('risk_score') for r in df_final if isinstance(r.get('risk_score'), (int, float))]\n",
    "    risk_score_p30 = _percentile(risk_scores, 0.30) if risk_scores else None\n",
    "\n",
    "    # Detección de escala de risk_score para validación\n",
    "    rs_min = min(risk_scores) if risk_scores else None\n",
    "    rs_max = max(risk_scores) if risk_scores else None\n",
    "    if rs_max is None:\n",
    "        risk_score_range = None\n",
    "    elif rs_max <= 150:\n",
    "        risk_score_range = (0, 150)\n",
    "    elif rs_min is not None and 250 <= rs_min <= 900 and rs_max <= 900:\n",
    "        risk_score_range = (250, 900)\n",
    "    else:\n",
    "        risk_score_range = None  # no validar si la escala es ambigua\n",
    "\n",
    "    # Helpers QC\n",
    "    def _is_num(x):\n",
    "        return isinstance(x, (int, float))\n",
    "\n",
    "    def _in_ranges(v, ranges):\n",
    "        return any((v >= a and v <= b) for (a, b) in ranges)\n",
    "\n",
    "    def _ts_in_snapshot_day(ts_str, snap_str):\n",
    "        try:\n",
    "            ts = datetime.strptime(ts_str, '%Y-%m-%d %H:%M:%S')\n",
    "            sd = datetime.strptime(snap_str, '%Y-%m-%d')\n",
    "            return ts.date() == sd.date()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    # Construir snapshot del día (cumplimiento de columnas requeridas)\n",
    "    todays_rows = []\n",
    "\n",
    "    def _status_final_from_row(row):\n",
    "        try:\n",
    "            if row.get('default_flag', 0) == 1:\n",
    "                return 'Default'\n",
    "            ls = row.get('loan_status')\n",
    "            if isinstance(ls, str):\n",
    "                lsl = ls.lower()\n",
    "                if ('paid' in lsl) or ('closed' in lsl):\n",
    "                    return 'Closed'\n",
    "            return 'Active'\n",
    "        except Exception:\n",
    "            return 'Active'\n",
    "\n",
    "    for row in df_final:\n",
    "        loan_identifier = row.get('loan_id')\n",
    "        if not loan_identifier:\n",
    "            loan_identifier = f\"L{hash(str(row)) % 100000:05d}\"\n",
    "\n",
    "        rate_pct = row.get('loan_int_rate') if _is_num(row.get('loan_int_rate')) else None\n",
    "        rate_m = (rate_pct / 100.0) / 12.0 if _is_num(rate_pct) and rate_pct > 0 else None\n",
    "        pay = row.get('monthly_payment_estimate') if _is_num(row.get('monthly_payment_estimate')) else None\n",
    "        P = row.get('loan_amnt') if _is_num(row.get('loan_amnt')) else None\n",
    "\n",
    "        loan_term_est = None\n",
    "        if rate_m is not None and pay is not None and pay > 0 and P is not None and P > 0:\n",
    "            x = 1 - (P * rate_m) / pay\n",
    "            if x > 0:\n",
    "                try:\n",
    "                    n = - math.log(x) / math.log(1 + rate_m)\n",
    "                    loan_term_est = int(math.ceil(n)) if n > 0 else None\n",
    "                    if loan_term_est is not None:\n",
    "                        loan_term_est = max(6, min(120, loan_term_est))\n",
    "                except Exception:\n",
    "                    loan_term_est = None\n",
    "\n",
    "        dti = row.get('debt_to_income_ratio') if _is_num(row.get('debt_to_income_ratio')) else None\n",
    "        lti = row.get('loan_to_income_ratio') if _is_num(row.get('loan_to_income_ratio')) else None\n",
    "        rs = row.get('risk_score') if _is_num(row.get('risk_score')) else None\n",
    "        high_risk_flag = 1 if (\n",
    "            (row.get('default_flag', 0) == 1) or\n",
    "            (_is_num(dti) and dti > 0.40) or\n",
    "            (_is_num(lti) and lti > 0.30) or\n",
    "            (_is_num(rate_pct) and rate_pct > 25.0) or\n",
    "            (risk_score_p30 is not None and _is_num(rs) and rs < risk_score_p30)\n",
    "        ) else 0\n",
    "\n",
    "        default_date = snapshot_date if row.get('default_flag', 0) == 1 else None\n",
    "        outstanding_at_default = row.get('outstanding_at_default')\n",
    "        if outstanding_at_default is None:\n",
    "            outstanding_at_default = row.get('loan_amnt') if row.get('default_flag', 0) == 1 else None\n",
    "\n",
    "        status_final = _status_final_from_row(row)\n",
    "\n",
    "        processing_ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        dashboard_row = {\n",
    "            'loan_id': loan_identifier,\n",
    "            'default_flag': row.get('default_flag'),\n",
    "            'loan_amnt': row.get('loan_amnt'),\n",
    "            'loan_int_rate': row.get('loan_int_rate'),\n",
    "            'debt_to_income_ratio': row.get('debt_to_income_ratio'),\n",
    "            'loan_to_income_ratio': row.get('loan_to_income_ratio'),\n",
    "            'person_age': row.get('person_age'),\n",
    "            'person_income': row.get('person_income'),\n",
    "            'person_home_ownership': row.get('person_home_ownership'),\n",
    "            'person_emp_length': row.get('person_emp_length'),\n",
    "            'loan_intent': row.get('loan_intent'),\n",
    "            'loan_grade': row.get('loan_grade'),\n",
    "            'risk_score': row.get('risk_score'),\n",
    "            'snapshot_date': snapshot_date,\n",
    "            'processing_timestamp': processing_ts,\n",
    "            'origination_date': row.get('origination_date'),\n",
    "            'closure_date': row.get('closure_date'),\n",
    "            'default_date': default_date,\n",
    "            'outstanding_at_default': outstanding_at_default,\n",
    "            'loss_amount': row.get('loss_amount'),\n",
    "            'recovered_amount': row.get('recovered_amount'),\n",
    "            'loan_term': row.get('loan_term'),\n",
    "            'loan_term_est': loan_term_est,\n",
    "            'status_final': status_final,\n",
    "            'geo_region': row.get('geo_region'),\n",
    "            'employment_type': row.get('employment_type'),\n",
    "            'industry': row.get('industry'),\n",
    "            'pep_flag': row.get('pep_flag'),\n",
    "            'high_risk_flag': high_risk_flag,\n",
    "            'cb_person_cred_hist_length': row.get('cb_person_cred_hist_length'),\n",
    "            'batch_id': batch_id,\n",
    "            'source_file': DATA_RAW_PATH,\n",
    "            'data_version': data_version,\n",
    "            'monthly_payment_estimate': (\n",
    "                row.get('loan_amnt') * (row.get('loan_int_rate') / 100 / 12)\n",
    "            ) if _is_num(row.get('loan_amnt')) and _is_num(row.get('loan_int_rate')) else None,\n",
    "        }\n",
    "\n",
    "        # Validaciones QC\n",
    "        qc_flags = []\n",
    "        if dashboard_row['default_flag'] not in (0, 1):\n",
    "            qc_flags.append('default_flag')\n",
    "        if not (_is_num(dashboard_row['loan_amnt']) and dashboard_row['loan_amnt'] > 0):\n",
    "            qc_flags.append('loan_amnt')\n",
    "        lir = dashboard_row['loan_int_rate']\n",
    "        if _is_num(lir):\n",
    "            if not (_in_ranges(lir, [(0, 1), (0, 60)])):\n",
    "                qc_flags.append('loan_int_rate')\n",
    "        else:\n",
    "            qc_flags.append('loan_int_rate')\n",
    "        dti_v = dashboard_row['debt_to_income_ratio']\n",
    "        if _is_num(dti_v):\n",
    "            if not (_in_ranges(dti_v, [(0, 1), (0, 100)])):\n",
    "                qc_flags.append('debt_to_income_ratio')\n",
    "        lti_v = dashboard_row['loan_to_income_ratio']\n",
    "        if _is_num(lti_v):\n",
    "            if not (0 <= lti_v <= 1.5):\n",
    "                qc_flags.append('loan_to_income_ratio')\n",
    "        if not (_is_num(dashboard_row['person_age']) and 18 <= dashboard_row['person_age'] <= 90):\n",
    "            qc_flags.append('person_age')\n",
    "        if not (_is_num(dashboard_row['person_income']) and dashboard_row['person_income'] > 0):\n",
    "            qc_flags.append('person_income')\n",
    "        pel = dashboard_row['person_emp_length']\n",
    "        if _is_num(pel) and pel < 0:\n",
    "            qc_flags.append('person_emp_length')\n",
    "        rs_v = dashboard_row['risk_score']\n",
    "        if _is_num(rs_v) and risk_score_range is not None:\n",
    "            if not (risk_score_range[0] <= rs_v <= risk_score_range[1]):\n",
    "                qc_flags.append('risk_score')\n",
    "        lte = dashboard_row.get('loan_term_est')\n",
    "        if lte is not None:\n",
    "            if not (_is_num(lte) and 6 <= lte <= 120):\n",
    "                qc_flags.append('loan_term_est')\n",
    "        if dashboard_row['default_flag'] == 1 and not dashboard_row['default_date']:\n",
    "            qc_flags.append('default_date_missing')\n",
    "        if dashboard_row['default_flag'] == 0 and dashboard_row['default_date']:\n",
    "            qc_flags.append('default_date_should_be_null')\n",
    "        try:\n",
    "            sd = datetime.strptime(dashboard_row['snapshot_date'], '%Y-%m-%d')\n",
    "            if sd.date() > datetime.now().date():\n",
    "                qc_flags.append('snapshot_date_future')\n",
    "        except Exception:\n",
    "            qc_flags.append('snapshot_date_invalid')\n",
    "        if not _ts_in_snapshot_day(dashboard_row['processing_timestamp'], dashboard_row['snapshot_date']):\n",
    "            qc_flags.append('processing_timestamp_out_of_day')\n",
    "        if not re.match(r'^ETL_\\d{8}_\\d{6}$', dashboard_row['batch_id']):\n",
    "            qc_flags.append('batch_id_pattern')\n",
    "        if dashboard_row['data_version'] != '1.0':\n",
    "            qc_flags.append('data_version')\n",
    "        if not dashboard_row['source_file']:\n",
    "            qc_flags.append('source_file')\n",
    "\n",
    "        dashboard_row['qc_error'] = 1 if qc_flags else 0\n",
    "        todays_rows.append(dashboard_row)\n",
    "\n",
    "    # Deduplicar por día\n",
    "    perday_dedup = {}\n",
    "    for r in todays_rows:\n",
    "        k = r.get('loan_id', '')\n",
    "        ts = r.get('processing_timestamp', '')\n",
    "        if k not in perday_dedup or ts > perday_dedup[k].get('processing_timestamp', ''):\n",
    "            perday_dedup[k] = r\n",
    "    todays_rows = list(perday_dedup.values())\n",
    "\n",
    "    # Validar duplicados (clave única loan_id + snapshot_date)\n",
    "    keys = [(r.get('loan_id', ''), r.get('snapshot_date', '')) for r in todays_rows]\n",
    "    dup_count = len(keys) - len(set(keys))\n",
    "\n",
    "    # Conteo de QC\n",
    "    qc_error_count = sum(1 for r in todays_rows if r.get('qc_error') == 1)\n",
    "    print(f\"   QC: filas con qc_error = {qc_error_count:,} de {len(todays_rows):,}\")\n",
    "    if dup_count > 0:\n",
    "        print(f\"   ADVERTENCIA: se detectaron {dup_count} duplicados de (loan_id, snapshot_date) antes de consolidar\")\n",
    "\n",
    "    # Escribir partición del día\n",
    "    partition_file = f\"{DATA_PROCESSED_PATH}/dashboard_partitions/snapshot_date={snapshot_date}.csv\"\n",
    "    write_csv_data(todays_rows, partition_file)\n",
    "    print(f\"   Partición escrita: {partition_file} (registros: {len(todays_rows):,})\")\n",
    "\n",
    "    # Consolidado: leer histórico si existe\n",
    "    existing_data = []\n",
    "    if Path(dashboard_file).exists():\n",
    "        print(\"   Aviso: Consolidado existente detectado - aplicando APPEND + DEDUP por (snapshot_date, loan_id)\")\n",
    "        with open(dashboard_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for r in reader:\n",
    "                existing_data.append(r)\n",
    "\n",
    "    combined = existing_data + todays_rows\n",
    "\n",
    "    # Deduplicación por (snapshot_date, loan_id) manteniendo mayor processing_timestamp\n",
    "    dedup = {}\n",
    "    for r in combined:\n",
    "        key = (r.get('snapshot_date', ''), r.get('loan_id', ''))\n",
    "        ts = r.get('processing_timestamp', '')\n",
    "        if key not in dedup or ts > dedup[key].get('processing_timestamp', ''):\n",
    "            dedup[key] = r\n",
    "\n",
    "    consolidated_rows = list(dedup.values())\n",
    "    consolidated_rows.sort(key=lambda x: (x.get('snapshot_date', ''), x.get('loan_id', '')))\n",
    "\n",
    "    # Derivación con snapshots (si hay >=2 días): origination_date, default_date (mínima con default), closure_date\n",
    "    unique_snapshots = sorted(set(r.get('snapshot_date', '') for r in consolidated_rows if r.get('snapshot_date')))\n",
    "    if len(unique_snapshots) >= 2:\n",
    "        # Mapas por loan_id\n",
    "        first_seen = {}\n",
    "        first_default = {}\n",
    "        first_closed = {}\n",
    "        for r in consolidated_rows:\n",
    "            lid = r.get('loan_id', '')\n",
    "            sdate = r.get('snapshot_date', '')\n",
    "            if not lid or not sdate:\n",
    "                continue\n",
    "            if lid not in first_seen:\n",
    "                first_seen[lid] = sdate\n",
    "            # Default\n",
    "            try:\n",
    "                df = int(r.get('default_flag', '0'))\n",
    "            except Exception:\n",
    "                df = 0\n",
    "            if df == 1:\n",
    "                if lid not in first_default:\n",
    "                    first_default[lid] = sdate\n",
    "            # Closed\n",
    "            if (r.get('status_final') == 'Closed') and (lid not in first_closed):\n",
    "                first_closed[lid] = sdate\n",
    "        # Aplicar derivaciones\n",
    "        for r in consolidated_rows:\n",
    "            lid = r.get('loan_id', '')\n",
    "            if lid in first_seen:\n",
    "                r['origination_date'] = first_seen[lid]\n",
    "            if lid in first_default:\n",
    "                r['default_date'] = first_default[lid]\n",
    "            if lid in first_closed:\n",
    "                r['closure_date'] = first_closed[lid]\n",
    "\n",
    "    # Escribir consolidado\n",
    "    write_csv_data(consolidated_rows, dashboard_file)\n",
    "\n",
    "    print(\"   Consolidado actualizado:\")\n",
    "    print(f\"      - Total histórico (únicos por fecha y préstamo): {len(consolidated_rows):,}\")\n",
    "    snapshots_in_file = len(set(r.get('snapshot_date', '') for r in consolidated_rows))\n",
    "    print(f\"      - Snapshots únicos en archivo: {snapshots_in_file}\")\n",
    "    print(\"   Recomendación: ejecutar ETL diariamente por 7-30 días para MTD/MoM/YoY\")\n",
    "\n",
    "    log_operation(f\"Se exportó dataset limpio: {output_file}\")\n",
    "    log_operation(f\"Partición escrita: {partition_file}\")\n",
    "    log_operation(f\"Consolidado actualizado: {dashboard_file}\")\n",
    "\n",
    "    print(\"\\nExportación completada exitosamente\")\n",
    "    print(\"Archivos listos para EDA y Dashboard\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en exportación: {e}\")\n",
    "    log_operation(f\"Error en exportación: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00051389",
   "metadata": {},
   "source": [
    "## 7. Generación de Reporte ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6f6b8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERACIÓN DE REPORTE ETL\n",
      "==================================================\n",
      "Reporte ETL generado:\n",
      "   - ../reports/etl_report_20251015_154322.md\n",
      "[2025-10-15 15:43:22] INFO: Se completó proceso ETL exitosamente\n",
      "\n",
      "🎉 PROCESO ETL COMPLETADO EXITOSAMENTE\n",
      "\n",
      "📊 Próximo paso: Ejecutar análisis exploratorio (EDA)\n",
      "📈 Dashboard data ready: ../data/processed/dashboard_data.csv\n",
      "📋 Reporte completo: ../reports/etl_report_20251015_154322.md\n"
     ]
    }
   ],
   "source": [
    "# Generar reporte completo del proceso ETL\n",
    "print(\"GENERACIÓN DE REPORTE ETL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calcular estadísticas finales\n",
    "original_rows = len(df_raw)\n",
    "final_rows = len(df_final)\n",
    "original_cols = len(df_raw[0].keys()) if df_raw else 0\n",
    "final_cols = len(df_final[0].keys()) if df_final else 0\n",
    "\n",
    "default_count = sum(1 for row in df_final if row['default_flag'] == 1)\n",
    "default_rate = default_count / len(df_final)\n",
    "\n",
    "# Generar contenido del reporte\n",
    "report_content = f\"\"\"# Reporte ETL - Analítica de Riesgo Crediticio\n",
    "\n",
    "## Información General\n",
    "- **Fecha de ejecución**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Archivo fuente**: {DATA_RAW_PATH}\n",
    "- **Archivo destino**: {output_file}\n",
    "\n",
    "## Resumen del Procesamiento\n",
    "\n",
    "### Datos Originales\n",
    "- **Filas**: {original_rows:,}\n",
    "- **Columnas**: {original_cols:,}\n",
    "- **Tipo**: Dataset sintético de riesgo crediticio\n",
    "\n",
    "### Datos Procesados\n",
    "- **Filas finales**: {final_rows:,}\n",
    "- **Columnas finales**: {final_cols:,}\n",
    "- **Retención de filas**: {(final_rows/original_rows*100):.1f}%\n",
    "- **Nuevas variables**: {final_cols - original_cols}\n",
    "\n",
    "## Variables Target Creadas\n",
    "\n",
    "### default_flag - Modelo PD (Probability of Default)\n",
    "- **Tipo**: Binaria (0/1)\n",
    "- **Default**: {default_count:,} registros ({default_rate:.1%})\n",
    "- **No Default**: {final_rows - default_count:,} registros ({(1-default_rate):.1%})\n",
    "- **Tasa de default**: {default_rate:.2%}\n",
    "\n",
    "### lgd_estimate - Modelo LGD (Loss Given Default)\n",
    "- **Tipo**: Continua (0.0-1.0)\n",
    "- **Basada en**: loan_grade (A=0.1, B=0.2, ..., G=0.7)\n",
    "- **Uso**: Estimación de pérdida en caso de default\n",
    "\n",
    "### ead_amount - Modelo EAD (Exposure at Default)\n",
    "- **Tipo**: Continua (monto en USD)\n",
    "- **Basada en**: loan_amnt\n",
    "- **Uso**: Exposición al momento del default\n",
    "\n",
    "## Variables Derivadas para Dashboard\n",
    "\n",
    "### Variables de Segmentación\n",
    "- **age_group**: 5 categorías (18-25, 26-35, 36-50, 51-65, 65+)\n",
    "- **income_bracket**: 3 categorías (Low ≤50K, Medium ≤100K, High >100K)\n",
    "- **person_home_ownership**: {len(set(row['person_home_ownership'] for row in df_final))} categorías\n",
    "\n",
    "### Variables KPI\n",
    "- **risk_score**: Score 0-100 basado en grade, historial, tasa de interés\n",
    "- **debt_to_income_ratio**: Ratio préstamo/ingreso\n",
    "- **loan_to_income_ratio**: Monto préstamo/ingreso anual\n",
    "- **monthly_payment_estimate**: Estimación pago mensual\n",
    "\n",
    "## Calidad de Datos\n",
    "\n",
    "### Validaciones Completadas\n",
    "- ✓ **Valores faltantes**: 0 encontrados\n",
    "- ✓ **Duplicados**: Verificación completada\n",
    "- ✓ **Tipos de datos**: Validados y convertidos\n",
    "- ✓ **Rangos de valores**: Dentro de parámetros esperados\n",
    "\n",
    "### Distribución de Variables Clave\n",
    "- **Edad promedio**: {sum(row['person_age'] for row in df_final)/len(df_final):.1f} años\n",
    "- **Ingreso promedio**: ${sum(row['person_income'] for row in df_final)/len(df_final):,.0f}\n",
    "- **Monto préstamo promedio**: ${sum(row['loan_amnt'] for row in df_final)/len(df_final):,.0f}\n",
    "- **Tasa interés promedio**: {sum(row['loan_int_rate'] for row in df_final)/len(df_final):.2f}%\n",
    "\n",
    "## Archivos Generados\n",
    "\n",
    "### Datos Principales\n",
    "- `{output_file}`: Dataset limpio principal\n",
    "- `{versioned_file}`: Dataset versionado con timestamp\n",
    "- `{DATA_PROCESSED_PATH}/dashboard_data.csv`: Dataset optimizado para dashboard\n",
    "\n",
    "### Logs y Metadatos\n",
    "- `../logs/etl_*.log`: Log detallado de ejecución\n",
    "- Schema JSON actualizado con nuevas variables\n",
    "\n",
    "## Preparación para Dashboard\n",
    "\n",
    "### Variables Listas para Visualización\n",
    "1. **KPIs Principales**:\n",
    "   - Tasa de Default por segmento\n",
    "   - Score de Riesgo promedio\n",
    "   - Distribución de LGD/EAD\n",
    "\n",
    "2. **Segmentaciones**:\n",
    "   - Por grupo etario\n",
    "   - Por bracket de ingresos\n",
    "   - Por tipo de propiedad\n",
    "   - Por intención del préstamo\n",
    "\n",
    "3. **Análisis Temporales**:\n",
    "   - Variables con timestamp de procesamiento\n",
    "   - Listas para análisis de tendencias\n",
    "\n",
    "4. **Métricas Financieras**:\n",
    "   - Ratio deuda/ingreso\n",
    "   - Estimación pagos mensuales\n",
    "   - Distribución de montos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "1. **EDA - Análisis Exploratorio**:\n",
    "   - Ejecutar notebook 02_eda.ipynb\n",
    "   - Análisis de correlaciones y patrones\n",
    "   - Identificación de segmentos de riesgo\n",
    "\n",
    "2. **Modelado Dimensional**:\n",
    "   - Revisar sql/modelado_decision.md\n",
    "   - Implementar esquema estrella para BI\n",
    "   - Crear vistas analíticas\n",
    "\n",
    "3. **Dashboard Development**:\n",
    "   - Usar dashboard_data.csv como fuente\n",
    "   - Implementar KPIs identificados\n",
    "   - Crear filtros por segmentación\n",
    "\n",
    "4. **Entrenamiento de Modelos**:\n",
    "   - Modelo PD: src/riskvista/models/train_pd.py\n",
    "   - Modelo LGD: variables lgd_estimate\n",
    "   - Modelo EAD: variables ead_amount\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "✅ **ETL COMPLETADO EXITOSAMENTE**\n",
    "\n",
    "- Se procesaron {final_rows:,} registros válidos\n",
    "- Se crearon {final_cols - original_cols} variables derivadas específicas para analítica de riesgo\n",
    "- Datos preparados para dashboard con variables de segmentación y KPIs\n",
    "- Calidad de datos validada y documentada\n",
    "- Variables target PD/LGD/EAD listas para modelado\n",
    "\n",
    "**Estado**: LISTO PARA EDA Y DASHBOARD\n",
    "\"\"\"\n",
    "\n",
    "# Escribir reporte a archivo\n",
    "report_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_file = f\"../reports/etl_report_{report_timestamp}.md\"\n",
    "\n",
    "# Crear directorio si no existe\n",
    "Path(\"../reports\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"Reporte ETL generado:\")\n",
    "print(f\"   - {report_file}\")\n",
    "\n",
    "# Crear resumen en JSON para uso programático\n",
    "summary_data = {\n",
    "    'execution_timestamp': datetime.now().isoformat(),\n",
    "    'source_file': DATA_RAW_PATH,\n",
    "    'output_file': output_file,\n",
    "    'dashboard_file': f\"{DATA_PROCESSED_PATH}/dashboard_data.csv\",\n",
    "    'original_rows': original_rows,\n",
    "    'final_rows': final_rows,\n",
    "    'original_columns': original_cols,\n",
    "    'final_columns': final_cols,\n",
    "    'default_rate': default_rate,\n",
    "    'variables_created': final_cols - original_cols,\n",
    "    'data_quality': {\n",
    "        'missing_values': 0,\n",
    "        'duplicates': 0,\n",
    "        'validation_passed': True\n",
    "    },\n",
    "    'next_steps': ['EDA', 'Dashboard', 'Modeling']\n",
    "}\n",
    "\n",
    "summary_file = f\"../reports/etl_summary_{report_timestamp}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "log_operation(\"Se completó proceso ETL exitosamente\", \"INFO\")\n",
    "print(f\"\\n🎉 PROCESO ETL COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"\\n📊 Próximo paso: Ejecutar análisis exploratorio (EDA)\")\n",
    "print(f\"📈 Dashboard data ready: {DATA_PROCESSED_PATH}/dashboard_data.csv\")\n",
    "print(f\"📋 Reporte completo: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3ade6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACIÓN DE ESQUEMA DASHBOARD\n",
      "==================================================\n",
      "Consolidado: ../data/processed/dashboard_data.csv\n",
      "   - Columnas faltantes: 0\n",
      "   - Esquema OK (todas las requeridas presentes)\n",
      "   - qc_error (consolidado): 0 de 4,863\n",
      "\n",
      "Partición del día: ../data/processed/dashboard_partitions/snapshot_date=2025-10-15.csv\n",
      "   - Columnas faltantes: 0\n",
      "   - Esquema OK (todas las requeridas presentes)\n",
      "   - qc_error (partición hoy): 0 de 4,863\n",
      "\n",
      "Validación de esquema completada.\n"
     ]
    }
   ],
   "source": [
    "# Validación de esquema de dashboard_data y partición diaria\n",
    "print(\"VALIDACIÓN DE ESQUEMA DASHBOARD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "required_cols = [\n",
    "    # 1) Mínimas obligatorias\n",
    "    'loan_id','default_flag','loan_amnt','loan_int_rate','debt_to_income_ratio','loan_to_income_ratio',\n",
    "    'person_age','person_income','person_home_ownership','person_emp_length','loan_intent','loan_grade','risk_score',\n",
    "    # 2) Tendencias/cohortes\n",
    "    'snapshot_date','processing_timestamp','origination_date','closure_date','default_date',\n",
    "    # 3) Observados PD/LGD/EAD\n",
    "    'outstanding_at_default','loss_amount','recovered_amount','loan_term','status_final',\n",
    "    # 4) Segmentación avanzada/SARLAFT\n",
    "    'geo_region','employment_type','industry','pep_flag','high_risk_flag','cb_person_cred_hist_length',\n",
    "    # 5) Auditoría\n",
    "    'batch_id','source_file','data_version','qc_error'\n",
    "]\n",
    "\n",
    "snapshot_date = datetime.now().strftime('%Y-%m-%d')\n",
    "dashboard_file = f\"{DATA_PROCESSED_PATH}/dashboard_data.csv\"\n",
    "partition_file = f\"{DATA_PROCESSED_PATH}/dashboard_partitions/snapshot_date={snapshot_date}.csv\"\n",
    "\n",
    "def _check_file(file_path):\n",
    "    if not Path(file_path).exists():\n",
    "        return False, [\"<archivo no existe>\"], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        headers = reader.fieldnames or []\n",
    "        rows = list(reader)\n",
    "    missing = [c for c in required_cols if c not in headers]\n",
    "    return True, missing, rows\n",
    "\n",
    "# Validar consolidado\n",
    "ok_c, missing_c, rows_c = _check_file(dashboard_file)\n",
    "print(f\"Consolidado: {dashboard_file}\")\n",
    "if not ok_c:\n",
    "    print(\"   - ERROR: archivo no encontrado\")\n",
    "else:\n",
    "    print(f\"   - Columnas faltantes: {len(missing_c)}\")\n",
    "    if missing_c:\n",
    "        print(f\"   - Faltantes: {missing_c}\")\n",
    "    else:\n",
    "        print(\"   - Esquema OK (todas las requeridas presentes)\")\n",
    "    # QC consolidado\n",
    "    qc_err_c = sum(1 for r in rows_c if r.get('qc_error') in ('1', 1))\n",
    "    print(f\"   - qc_error (consolidado): {qc_err_c:,} de {len(rows_c):,}\")\n",
    "    # Duplicados consolidado\n",
    "    keys_c = [(r.get('loan_id',''), r.get('snapshot_date','')) for r in rows_c]\n",
    "    dup_count_c = len(keys_c) - len(set(keys_c))\n",
    "    if dup_count_c > 0:\n",
    "        print(f\"   - ADVERTENCIA: {dup_count_c} duplicados en consolidado\")\n",
    "        # Exportar duplicados\n",
    "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        dup_path = f\"{ERRORS_PATH}/duplicates_consolidated_{ts}.csv\"\n",
    "        # Reconstruir headers\n",
    "        hdrs = list({k for r in rows_c for k in r.keys()})\n",
    "        seen = set()\n",
    "        with open(dup_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            w = csv.DictWriter(f, fieldnames=hdrs)\n",
    "            w.writeheader()\n",
    "            for r in rows_c:\n",
    "                key = (r.get('loan_id',''), r.get('snapshot_date',''))\n",
    "                if key in seen:\n",
    "                    w.writerow(r)\n",
    "                else:\n",
    "                    seen.add(key)\n",
    "        print(f\"   - Duplicados exportados: {dup_path}\")\n",
    "\n",
    "# Validar partición del día\n",
    "ok_p, missing_p, rows_p = _check_file(partition_file)\n",
    "print(f\"\\nPartición del día: {partition_file}\")\n",
    "if not ok_p:\n",
    "    print(\"   - ADVERTENCIA: partición del día no encontrada (ejecutar exportación)\")\n",
    "else:\n",
    "    print(f\"   - Columnas faltantes: {len(missing_p)}\")\n",
    "    if missing_p:\n",
    "        print(f\"   - Faltantes: {missing_p}\")\n",
    "    else:\n",
    "        print(\"   - Esquema OK (todas las requeridas presentes)\")\n",
    "    # QC partición\n",
    "    qc_err_p = sum(1 for r in rows_p if r.get('qc_error') in ('1', 1))\n",
    "    print(f\"   - qc_error (partición hoy): {qc_err_p:,} de {len(rows_p):,}\")\n",
    "    # Duplicados partición\n",
    "    keys_p = [(r.get('loan_id',''), r.get('snapshot_date','')) for r in rows_p]\n",
    "    dup_count_p = len(keys_p) - len(set(keys_p))\n",
    "    if dup_count_p > 0:\n",
    "        print(f\"   - ADVERTENCIA: {dup_count_p} duplicados en partición del día\")\n",
    "\n",
    "print(\"\\nValidación de esquema completada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
