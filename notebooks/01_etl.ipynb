{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a9e704",
   "metadata": {},
   "source": [
    "# ETL - Analítica de Riesgo Crediticio\n",
    "\n",
    "## Objetivo\n",
    "Realizar la extracción, transformación y carga (ETL) del dataset `Loan_default.csv` para preparar los datos para el análisis de riesgo crediticio y modelado PD/LGD/EAD.\n",
    "\n",
    "## Entregables\n",
    "- `data/processed/clean_data.csv`: Dataset limpio y procesado\n",
    "- `reports/etl_report.md`: Reporte detallado del proceso ETL\n",
    "- `data/errors/etl_errors_*.csv`: Registros con errores identificados\n",
    "- `logs/etl_*.log`: Logs de ejecución\n",
    "\n",
    "## Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2599a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Importar módulos del proyecto\n",
    "from src.riskvista.utils.reporting import setup_logging, write_report, log_operation, save_error_rows\n",
    "from src.riskvista.utils.data_processing import DataIngestion, DataCleaning\n",
    "\n",
    "# Configurar logging\n",
    "logger = setup_logging(\"logs\", \"INFO\")\n",
    "log_operation(\"Iniciando proceso ETL\", \"INFO\")\n",
    "\n",
    "print(\"✅ Configuración inicial completada\")\n",
    "print(f\"📁 Directorio de trabajo: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c89f7",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4738ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar rutas\n",
    "DATA_RAW_PATH = \"data/raw/Loan_default.csv\"\n",
    "DATA_PROCESSED_PATH = \"data/processed\"\n",
    "ERRORS_PATH = \"data/errors\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "Path(DATA_PROCESSED_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(ERRORS_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verificar existencia del archivo\n",
    "if not Path(DATA_RAW_PATH).exists():\n",
    "    print(f\"❌ ERROR: Archivo {DATA_RAW_PATH} no encontrado\")\n",
    "    print(\"📋 Por favor, colocar el archivo Loan_default.csv en la carpeta data/raw/\")\n",
    "    print(\"🔄 El proceso ETL se pausará hasta que el archivo esté disponible\")\n",
    "else:\n",
    "    print(f\"✅ Archivo encontrado: {DATA_RAW_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13817d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos usando DataIngestion\n",
    "ingestion = DataIngestion()\n",
    "\n",
    "try:\n",
    "    df_raw = ingestion.load_loan_data(DATA_RAW_PATH)\n",
    "    log_operation(f\"Datos cargados exitosamente: {df_raw.shape}\")\n",
    "    \n",
    "    print(f\"📊 Dataset cargado:\")\n",
    "    print(f\"   - Filas: {df_raw.shape[0]:,}\")\n",
    "    print(f\"   - Columnas: {df_raw.shape[1]:,}\")\n",
    "    print(f\"   - Memoria: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error cargando datos: {e}\")\n",
    "    log_operation(f\"Error en carga: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966b597",
   "metadata": {},
   "source": [
    "## 2. Exploración Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información básica del dataset\n",
    "print(\"📋 INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(df_raw.info())\n",
    "\n",
    "print(\"\\n📊 PRIMERAS FILAS\")\n",
    "print(\"=\" * 50)\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\n📈 ESTADÍSTICAS DESCRIPTIVAS (Variables Numéricas)\")\n",
    "print(\"=\" * 50)\n",
    "display(df_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores faltantes\n",
    "print(\"🔍 ANÁLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Column': df_raw.columns,\n",
    "    'Missing_Count': df_raw.isnull().sum(),\n",
    "    'Missing_Percentage': (df_raw.isnull().sum() / len(df_raw)) * 100,\n",
    "    'Data_Type': df_raw.dtypes\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Filtrar columnas con valores faltantes\n",
    "missing_stats = missing_stats[missing_stats['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_stats) > 0:\n",
    "    display(missing_stats)\n",
    "    \n",
    "    # Identificar columnas críticas con muchos faltantes\n",
    "    critical_missing = missing_stats[missing_stats['Missing_Percentage'] > 50]\n",
    "    if len(critical_missing) > 0:\n",
    "        print(f\"\\n⚠️  COLUMNAS CON >50% FALTANTES ({len(critical_missing)}):\")\n",
    "        for col in critical_missing['Column']:\n",
    "            print(f\"   - {col}: {critical_missing[critical_missing['Column']==col]['Missing_Percentage'].iloc[0]:.1f}%\")\n",
    "else:\n",
    "    print(\"✅ No se encontraron valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dec736",
   "metadata": {},
   "source": [
    "## 3. Validación y Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebad93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar limpiador de datos\n",
    "cleaner = DataCleaning()\n",
    "\n",
    "# 1. Limpiar loan_status\n",
    "print(\"🧹 LIMPIEZA DE LOAN_STATUS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Estados originales:\")\n",
    "print(df_raw['loan_status'].value_counts())\n",
    "\n",
    "df_clean = cleaner.clean_loan_status(df_raw)\n",
    "\n",
    "print(\"\\nEstados después de limpieza:\")\n",
    "print(df_clean['loan_status_clean'].value_counts())\n",
    "\n",
    "log_operation(\"Loan_status limpiado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear variables target\n",
    "print(\"\\n🎯 CREACIÓN DE VARIABLES TARGET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_clean = cleaner.create_target_variables(df_clean)\n",
    "\n",
    "# Verificar creación de variables target\n",
    "target_vars = ['default_flag', 'loss_rate', 'ead_ratio']\n",
    "created_targets = [var for var in target_vars if var in df_clean.columns]\n",
    "\n",
    "print(f\"Variables target creadas: {created_targets}\")\n",
    "\n",
    "for var in created_targets:\n",
    "    if var == 'default_flag':\n",
    "        print(f\"\\n{var} - Distribución:\")\n",
    "        print(df_clean[var].value_counts())\n",
    "        print(f\"Tasa de default: {df_clean[var].mean():.2%}\")\n",
    "    else:\n",
    "        print(f\"\\n{var} - Estadísticas:\")\n",
    "        print(df_clean[var].describe())\n",
    "\n",
    "log_operation(f\"Variables target creadas: {created_targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Manejar valores faltantes\n",
    "print(\"\\n🔧 MANEJO DE VALORES FALTANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar estrategia de limpieza\n",
    "missing_strategy = {\n",
    "    'numerical': 'median',\n",
    "    'categorical': 'mode',\n",
    "    'drop_threshold': 0.7  # Eliminar columnas con >70% faltantes\n",
    "}\n",
    "\n",
    "# Guardar filas con problemas antes de limpiar\n",
    "problematic_rows = df_clean[df_clean.isnull().any(axis=1)].copy()\n",
    "if len(problematic_rows) > 0:\n",
    "    save_error_rows(problematic_rows, \"initial_missing_values\", ERRORS_PATH)\n",
    "    print(f\"💾 Guardadas {len(problematic_rows)} filas con valores faltantes en data/errors/\")\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_clean = cleaner.handle_missing_values(df_clean, missing_strategy)\n",
    "\n",
    "print(f\"✅ Limpieza completada:\")\n",
    "print(f\"   - Filas restantes: {len(df_clean):,}\")\n",
    "print(f\"   - Columnas restantes: {df_clean.shape[1]:,}\")\n",
    "print(f\"   - Valores faltantes restantes: {df_clean.isnull().sum().sum():,}\")\n",
    "\n",
    "log_operation(f\"Manejo de faltantes completado. Filas finales: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Detección y manejo de outliers\n",
    "print(\"\\n📊 DETECCIÓN DE OUTLIERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Seleccionar columnas numéricas críticas para detección de outliers\n",
    "numeric_cols = ['loan_amnt', 'funded_amnt', 'int_rate', 'installment', 'annual_inc', 'dti']\n",
    "available_cols = [col for col in numeric_cols if col in df_clean.columns]\n",
    "\n",
    "print(f\"Analizando outliers en: {available_cols}\")\n",
    "\n",
    "df_no_outliers, df_outliers = cleaner.handle_outliers(\n",
    "    df_clean, \n",
    "    columns=available_cols, \n",
    "    method='iqr', \n",
    "    threshold=2.0  # Más conservador para datos financieros\n",
    ")\n",
    "\n",
    "# Guardar outliers identificados\n",
    "if len(df_outliers) > 0:\n",
    "    save_error_rows(df_outliers, \"outliers_detected\", ERRORS_PATH)\n",
    "    print(f\"💾 Guardados {len(df_outliers)} outliers en data/errors/\")\n",
    "\n",
    "# Decidir si mantener outliers (común en datos financieros)\n",
    "print(f\"\\n📈 Análisis de outliers:\")\n",
    "print(f\"   - Outliers detectados: {len(df_outliers):,} ({len(df_outliers)/len(df_clean):.1%})\")\n",
    "print(f\"   - Datos sin outliers: {len(df_no_outliers):,}\")\n",
    "\n",
    "# Para riesgo crediticio, generalmente mantenemos outliers pero los documentamos\n",
    "df_final = df_clean.copy()  # Mantener todos los datos\n",
    "print(\"✅ Decisión: Mantener outliers documentados (típico en riesgo crediticio)\")\n",
    "\n",
    "log_operation(f\"Outliers detectados y documentados: {len(df_outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2fa92",
   "metadata": {},
   "source": [
    "## 4. Ingeniería de Características Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear features derivadas básicas\n",
    "print(\"⚙️ CREACIÓN DE FEATURES DERIVADAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_final = cleaner.create_derived_features(df_final)\n",
    "\n",
    "# Identificar nuevas features creadas\n",
    "new_features = [col for col in df_final.columns if col.endswith(('_ratio', '_length', '_avg', '_amount', '_installment'))]\n",
    "\n",
    "print(f\"✅ Features derivadas creadas ({len(new_features)}):\")\n",
    "for feature in new_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "# Estadísticas de las nuevas features\n",
    "if new_features:\n",
    "    print(f\"\\n📊 Estadísticas de features derivadas:\")\n",
    "    display(df_final[new_features].describe())\n",
    "\n",
    "log_operation(f\"Features derivadas creadas: {len(new_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fa9ba",
   "metadata": {},
   "source": [
    "## 5. Validación Final y Exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9121383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación final de calidad de datos\n",
    "print(\"✅ VALIDACIÓN FINAL DE CALIDAD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar integridad de datos críticos\n",
    "validation_results = {\n",
    "    'total_rows': len(df_final),\n",
    "    'total_columns': df_final.shape[1],\n",
    "    'missing_values': df_final.isnull().sum().sum(),\n",
    "    'duplicate_rows': df_final.duplicated().sum(),\n",
    "    'memory_usage_mb': df_final.memory_usage(deep=True).sum() / 1024**2\n",
    "}\n",
    "\n",
    "print(\"📊 Resumen final del dataset:\")\n",
    "for key, value in validation_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   - {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   - {key.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "# Validaciones específicas de riesgo crediticio\n",
    "print(f\"\\n🎯 Validaciones específicas:\")\n",
    "\n",
    "# 1. Variable target principal\n",
    "if 'default_flag' in df_final.columns:\n",
    "    default_rate = df_final['default_flag'].mean()\n",
    "    print(f\"   - Tasa de default: {default_rate:.2%}\")\n",
    "    if default_rate < 0.01 or default_rate > 0.50:\n",
    "        print(f\"   ⚠️  Tasa de default inusual: {default_rate:.2%}\")\n",
    "\n",
    "# 2. Distribución de montos\n",
    "if 'loan_amnt' in df_final.columns:\n",
    "    loan_stats = df_final['loan_amnt'].describe()\n",
    "    print(f\"   - Monto promedio de préstamo: ${loan_stats['mean']:,.0f}\")\n",
    "    print(f\"   - Rango de montos: ${loan_stats['min']:,.0f} - ${loan_stats['max']:,.0f}\")\n",
    "\n",
    "# 3. Calidad de dates\n",
    "date_columns = df_final.select_dtypes(include=['datetime64']).columns\n",
    "print(f\"   - Columnas de fecha válidas: {len(date_columns)}\")\n",
    "\n",
    "log_operation(\"Validación final completada\", \"INFO\", validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar dataset limpio\n",
    "print(\"\\n💾 EXPORTACIÓN DE DATOS LIMPIOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Archivo principal\n",
    "output_file = f\"{DATA_PROCESSED_PATH}/clean_data.csv\"\n",
    "df_final.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "# Archivo con timestamp para versionado\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_file = f\"{DATA_PROCESSED_PATH}/clean_data_{timestamp}.csv\"\n",
    "df_final.to_csv(versioned_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✅ Dataset exportado:\")\n",
    "print(f\"   - Archivo principal: {output_file}\")\n",
    "print(f\"   - Archivo versionado: {versioned_file}\")\n",
    "print(f\"   - Tamaño: {Path(output_file).stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "log_operation(f\"Dataset limpio exportado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be62fe",
   "metadata": {},
   "source": [
    "## 6. Generación de Reporte ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd204be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo del proceso ETL\n",
    "report_content = f\"\"\"# Reporte ETL - Analítica de Riesgo Crediticio\n",
    "\n",
    "## Información General\n",
    "- **Fecha de ejecución**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Archivo fuente**: {DATA_RAW_PATH}\n",
    "- **Archivo destino**: {output_file}\n",
    "\n",
    "## Resumen del Procesamiento\n",
    "\n",
    "### Datos Originales\n",
    "- **Filas**: {df_raw.shape[0]:,}\n",
    "- **Columnas**: {df_raw.shape[1]:,}\n",
    "- **Tamaño**: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\n",
    "\n",
    "### Datos Procesados\n",
    "- **Filas finales**: {df_final.shape[0]:,}\n",
    "- **Columnas finales**: {df_final.shape[1]:,}\n",
    "- **Tamaño final**: {df_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\n",
    "- **Reducción de filas**: {((df_raw.shape[0] - df_final.shape[0]) / df_raw.shape[0] * 100):.1f}%\n",
    "\n",
    "## Variables Target Creadas\n",
    "\n",
    "### default_flag (PD Model)\n",
    "- **Tipo**: Binaria (0/1)\n",
    "- **Distribución**: {df_final['default_flag'].value_counts().to_dict() if 'default_flag' in df_final.columns else 'No creada'}\n",
    "- **Tasa de default**: {df_final['default_flag'].mean():.2%} if 'default_flag' in df_final.columns else 'N/A'}\n",
    "\n",
    "### loss_rate (LGD Model)\n",
    "- **Tipo**: Continua (0-1)\n",
    "- **Observaciones válidas**: {df_final['loss_rate'].notna().sum() if 'loss_rate' in df_final.columns else 'No creada'}\n",
    "- **Pérdida promedio**: {df_final['loss_rate'].mean():.2%} if 'loss_rate' in df_final.columns else 'N/A'}\n",
    "\n",
    "## Calidad de Datos\n",
    "\n",
    "### Valores Faltantes\n",
    "- **Antes del procesamiento**: {df_raw.isnull().sum().sum():,}\n",
    "- **Después del procesamiento**: {df_final.isnull().sum().sum():,}\n",
    "- **Columnas eliminadas por alta faltanza**: {df_raw.shape[1] - df_final.shape[1]}\n",
    "\n",
    "### Outliers\n",
    "- **Método de detección**: IQR con umbral 2.0\n",
    "- **Outliers identificados**: {len(df_outliers) if 'df_outliers' in locals() else 0:,}\n",
    "- **Acción**: Documentados pero mantenidos en dataset\n",
    "\n",
    "## Features Derivadas\n",
    "\n",
    "### Features Creadas ({len(new_features) if 'new_features' in locals() else 0})\n",
    "\"\"\"\n",
    "\n",
    "if 'new_features' in locals() and new_features:\n",
    "    for feature in new_features:\n",
    "        report_content += f\"\\n- **{feature}**\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "\n",
    "## Archivos Generados\n",
    "\n",
    "### Datos Principales\n",
    "- `{output_file}`: Dataset limpio principal\n",
    "- `{versioned_file}`: Dataset versionado con timestamp\n",
    "\n",
    "### Archivos de Error\n",
    "- `data/errors/initial_missing_values_errors_*.csv`: Registros con valores faltantes\n",
    "- `data/errors/outliers_detected_errors_*.csv`: Outliers identificados\n",
    "\n",
    "### Logs\n",
    "- `logs/riskvista_*.log`: Log detallado de ejecución\n",
    "\n",
    "## Validaciones de Riesgo Crediticio\n",
    "\n",
    "### Coherencia de Datos\n",
    "✅ Tasa de default dentro de rangos esperados  \n",
    "✅ Distribución de montos de préstamo coherente  \n",
    "✅ Variables de fecha válidas  \n",
    "✅ No hay registros duplicados  \n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "1. **EDA (Análisis Exploratorio)**:\n",
    "   - Ejecutar `notebooks/02_eda.ipynb`\n",
    "   - Análisis de distribuciones y correlaciones\n",
    "   - Identificación de patrones de riesgo\n",
    "\n",
    "2. **Modelado Dimensional**:\n",
    "   - Revisar `sql/modelado_decision.md`\n",
    "   - Aprobar estructura de base de datos\n",
    "   - Crear tablas dimensionales\n",
    "\n",
    "3. **Entrenamiento de Modelos**:\n",
    "   - Modelo PD: `src/riskvista/models/train_pd.py`\n",
    "   - Modelo LGD: `src/riskvista/models/train_lgd.py`\n",
    "   - Modelo EAD: `src/riskvista/models/train_ead.py`\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "El proceso ETL se completó exitosamente. Los datos están listos para el análisis exploratorio y el entrenamiento de modelos de riesgo crediticio. Se mantuvieron {df_final.shape[0]:,} registros válidos para el análisis.\n",
    "\n",
    "**Estado**: ✅ ETL COMPLETADO - LISTO PARA EDA\n",
    "\"\"\"\n",
    "\n",
    "# Guardar reporte\n",
    "report_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "write_report(report_content, f\"etl_report_{report_timestamp}\", \"reports\")\n",
    "\n",
    "print(\"📄 Reporte ETL generado:\")\n",
    "print(f\"   - reports/etl_report_{report_timestamp}.md\")\n",
    "\n",
    "log_operation(\"Proceso ETL completado exitosamente\", \"INFO\")\n",
    "print(\"\\n🎉 ¡PROCESO ETL COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"\\n📋 Próximo paso: Ejecutar análisis exploratorio (EDA)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
